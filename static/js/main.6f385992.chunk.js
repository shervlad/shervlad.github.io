(this.webpackJsonpwebsite=this.webpackJsonpwebsite||[]).push([[0],{32:function(e,t,a){},43:function(e,t,a){e.exports=a(60)},48:function(e,t,a){},49:function(e,t,a){},60:function(e,t,a){"use strict";a.r(t);var n=a(0),r=a.n(n),i=a(19),l=a.n(i),o=(a(48),a(49),a(7)),s=a(8),c=a(10),h=a(9),m=a(62),d=a(63),u=a(64),p=a(41),g=a(6),f=a(3),w=a(65),b=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement("img",{style:{width:"50%",float:"right"},src:"/maze_solver/wall3d.png"}),r.a.createElement("p",null,'The Maze Challenge was the cultimation of the "Robotics: Science and Systems" class. The challenge is to program Wall3d (the robot in the image) to get out of an unexplored maze as fast as possible while minimizing collisions.'),r.a.createElement("p",null,"To accomplish this task, Wall3D uses SLAM, path planning, trajectory following, safety controllers, and frontier exploration."),r.a.createElement("p",null,"Our robot got the best score in the Maze Challenge. Most other approaches relied on some variation of wall following."),r.a.createElement("p",null,"Here is a video that goes through the high level explanation of how our robot works:"),r.a.createElement("div",{className:"video-wrapper"},r.a.createElement("iframe",{className:"video-frame",src:"https://www.youtube.com/embed/PrP86YrPoR0",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})),r.a.createElement("p",null,"The sildes for more details on how we use Google Cartografer for SLAM and how we process the map in order to be able to perform A*, and subsequently trajectory following, efficiently:"),r.a.createElement("div",{className:"video-wrapper"},r.a.createElement("iframe",{className:"video-frame",src:"https://docs.google.com/presentation/d/e/2PACX-1vTqXaYhTXgwJ_DPO3bpJjTPzB6wHcrB4mNkEAurd-2MHXkMApfmxKwZkfwkg2wChygbciwCbMuz_3Cd/embed?start=false&loop=false&delayms=3000",frameborder:"0",width:"960",height:"569",allowfullscreen:"true",mozallowfullscreen:"true",webkitallowfullscreen:"true"})),r.a.createElement("p",null,"One very important thing we learned in this class is that a robot is a complex system, and it can fail in many unexpected ways, as seen in this video:"),r.a.createElement("div",{className:"video-wrapper"},r.a.createElement("iframe",{className:"video-frame",src:"https://www.youtube.com/embed/bDDuAJvdg-M",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})),r.a.createElement("p",null,"Nevertheless, after dozens of hours of relentless tweaking, we got pretty good results. This was our final run:"),r.a.createElement("div",{style:{width:"100%",position:"relative"}},r.a.createElement("div",null,r.a.createElement("div",{style:{width:"50%",display:"inline-block"}},"Rviz visualization:"),r.a.createElement("div",{style:{width:"50%",display:"inline-block"}},"Reality:")),r.a.createElement("div",null,r.a.createElement("div",{style:{float:"left",width:"50%"}},r.a.createElement("iframe",{width:"100%",src:"https://www.youtube.com/embed/Otesxus6TXY",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})),r.a.createElement("div",{style:{float:"right",width:"50%"}},r.a.createElement("iframe",{width:"100%",src:"https://www.youtube.com/embed/alsAfsBNwQI",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})))),r.a.createElement("h4",null,"Conclusion"),r.a.createElement("p",null,"RSS was one of the best classes I took at MIT. It was very demanding (aprox. 20 hours/week on average) but the amount of knowledge per hour learned in this class was unparalleled. Most of the time was spent together with the team working on lab assignments. I was personally responsible for slam (using cartographer) and path planning (A*). Other members of the team were responsible for safety controllers, path following (pure pursuit) and a plan B - a simpler implementation based on wall-following."),r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/team.jpg"}))}}]),a}(n.Component),v=(a(32),function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement("h5",null,"Table of Contents:"),r.a.createElement("ol",null,r.a.createElement("li",null,r.a.createElement("a",{href:"#1"},"Overview and Motivations")),r.a.createElement("li",null,r.a.createElement("a",{href:"#2"},"Proposed Approach"),r.a.createElement("ol",{type:"a"},r.a.createElement("li",null,r.a.createElement("a",{href:"#2a"},"Frontier exploration")),r.a.createElement("li",null,r.a.createElement("a",{href:"#2b"},"Path planning")),r.a.createElement("li",null,r.a.createElement("a",{href:"#2c"},"Trajectory tracking")))),r.a.createElement("li",null,r.a.createElement("a",{href:"#3"},"Experimental Evaluation")),r.a.createElement("li",null,r.a.createElement("a",{href:"#4"},"Lessons Learned")),r.a.createElement("li",null,r.a.createElement("a",{href:"#5"},"Potential Improvements"))),r.a.createElement("div",null,r.a.createElement("h4",{id:"1"}," 1. OVERVIEW AND MOTIVATIONS "),r.a.createElement("div",null,r.a.createElement("img",{className:"halfscreen",src:"/maze_solver/maze_example.png"}),r.a.createElement("p",null,"The labyrinth final challenge involves the car being placed in an unknown maze of a given radius and having to exit the maze in a time constraint. Figure 1 illustrates an example maze that could be used. This challenge uses simultaneous localization and mapping from the localization lab so the car can generate a map of the maze as it drives and keep track of where it is in the map. This challenge also utilizes a search-based path planning algorithm from the path planning lab to create a path that represents a solution to the maze and uses the pure pursuit algorithm from the same path planning lab to follow the solution path to the exit. It is given that the maze is multicursal and not simply connected, so the car is not guaranteed to exit the maze if it uses the basic maze solving method of following one wall. The main difficulties our team is anticipating when approaching this challenge includes how to efficiently explore the map and how to ensure the car can maneuver in tight spaces (i.e. 3-point turns when leaving dead ends).")),r.a.createElement("h4",{id:"2"},"2. PROPOSED APPROACH "),r.a.createElement("p",null,"To solve the maze, we use frontier exploration to generate a map of the maze and once the car detects an area outside the radius of the maze, a path is generated for the car to follow to complete the maze. To search frontiers, our priority queue is based on proximity to the car\u2019s current location because search algorithms such as breadth-first search and depth-first search require the car to make giant leaps from one frontier to another which is not instantaneous in real life. To follow the paths generated via Dijkstra\u2019s algorithm from the car\u2019s location to the nearest frontier, the car uses pure pursuit both forwards and backwards, depending on which direction is more optimal, considering the maze has fairly tight hallways."),r.a.createElement("h5",{id:"2a"},"2.a FRONTIER EXPLORATION"),r.a.createElement("b",null,"Processing the map"),r.a.createElement("div",null,r.a.createElement("div",null,r.a.createElement("div",{className:"halfscreen"},r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/thin_walls.png"}),r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/thick_walls.png"}))),r.a.createElement("p",null,"Our team uses Google Cartographer for simultaneous localization and mapping. Cartographer outputs a grid of 20 cells/meter, each cell being assigned a value of -1 if it is \u201cunexplored\u201d or a value from 0 to 100 representing the probability of that cell being occupied. To make this map more useful and easy to work with, we downsampled it to about 10 cm/cell. We then converted it to a true occupancy grid that only contains three values: -1, 0, and 100 for \u201cunexplored\u201d, free space, and occupied, respectively. Additionally, we \u201cthickened\u201d the walls and assigned a cost to each cell based on its distance from a wall in order to avoid wall collisions.")),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("b",null,"Downsampling and converting the map into a true occupancy grid."),r.a.createElement("p",null,"First, we define a downsampling factor. Let\u2019s call it d. Each cell in the downsampled map is d x d cells in the original map. We consider a cell in the original map to represent a wall if its value is greater than 50. If at least 10% of the \u201coriginal\u201d cells in the downsampled cell are occupied (value > 50), then we mark the downsampled cell as occupied as well.")),r.a.createElement("li",null,r.a.createElement("b",null,"Thickening the walls"),r.a.createElement("p",null,"In order to \u201cthicken\u201d the walls on the map, we defined a procedure that creates a copy of the map, iterates through every cell, and if the cell has at least two occupied neighboring cells on the original map, the cell is marked as occupied as well. At the end, the copy is saved and the old map is removed. We repeat this process three times in order to ensure our map is sufficiently dilated.")),r.a.createElement("li",null,r.a.createElement("b",null,"Assigning costs to the cells on the map "),r.a.createElement("p",null,"If one just runs BFS on an occupancy grid, the shortest path will have sections that are very close to walls. In order to avoid this, we assign a cost to each cell based on its distance to the closest wall. The closer to a wall a cell is, the higher the associated cost. The cost of a cell we use is 2^20/2^(# cells from wall). Any cell that is more than 20 cells away from the nearest wall is assigned a cost of 0."))),r.a.createElement("h5",{id:"2b"},"2.b PATH PLANNING"),r.a.createElement("div",null,r.a.createElement("div",null,r.a.createElement("div",{className:"halfscreen"},r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/djikstra.gif"}))),r.a.createElement("p",null,"For path planning, we use Dijkstra\u2019s algorithm. The algorithm returns the path of minimal cost to the closest \u201cunexplored\u201d cell (with value -1) from the current position of the car. The cost of a path is the sum of the costs of the cells along it."),r.a.createElement("p",null,"Dijkstra\u2019s algorithm performs frontier exploration until it finds a destination. It always expands the frontier to the point of minimal distance, or cost, from the starting point."),r.a.createElement("p",null,"Due to our lidar not being able to see backwards, it is highly preferable that the paths generated be directed forward. Therefore, we draw an imaginary wall behind the car on the map and then run our path finding algorithm. If the algorithm can\u2019t find any path, we remove the wall and try generating a path again.")),r.a.createElement("h5",{id:"2c"},"2.c TRAJECTORY TRACKING"),r.a.createElement("p",null,"For trajectory tracking, we use the pure pursuit algorithm from the path planning lab. To reiterate from last lab, the pure pursuit algorithm begins by finding the segment in the trajectory that is closest to the car. Then starting from the closest segment, the algorithm iterates through the rest of the segments in the trajectory until a segment that contains a point exactly one \u201clookahead distance\u201d away from the car is found. The steering angle commanded to the car to follow the trajectory then becomes a function of the lookahead distance, the angle from the line segment made from the goal point and the car position to the car, and the car\u2019s length. The geometry and equations used for the pure pursuit algorithm are illustrated in the image below."),r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/pure_pursuit.png"}),r.a.createElement("p",null,"Since the maze has tight hallways, the car cannot always make a full turn to start following a trajectory forwards without hitting a wall. This is especially the case when the trajectory and the car are facing in opposite directions as shown on the right of Figure ___. One possible solution to this issue is to implement a three-point turn every time the car needs to turn around. However, our car uses a Velodyne lidar to collect its scan data, and this lidar cannot detect objects within 0.5 meters. This means that if the car were to begin a three-point turn, it would not be able to detect how close it is to a wall in order to change the direction of driving."),r.a.createElement("div",null,r.a.createElement("div",null,r.a.createElement("div",{className:"halfscreen"},r.a.createElement("img",{style:{width:"100%"},src:"/maze_solver/forwards_backwards.png"}))),r.a.createElement("p",null,"Therefore, we decided to incorporate pure pursuit in reverse whenever the trajectory was behind the car. All of the calculations for pure pursuit in reverse are the same as before, except the sign of the final steering angle and speed the car is commanded to drive is reversed. The metric we use in order to determine if the trajectory is in front or behind the car was the difference in angle between the first segment of the trajectory and the car\u2019s pose, both in the world frame. As shown in figure __, if the difference in angle is more than \u03c0/2, then that means the car is facing in the opposite direction of the trajectory, and pure pursuit in reverse should be used over the regular pure pursuit algorithm.")),r.a.createElement("h4",{id:"3"},"3. EXPERIMENTAL EVALUATION"),r.a.createElement("p",null,"We chose to focus our testing of the Maze Solver on real life scenarios. Due to the increased accuracy of Google Cartographer in simulation and the near-sightedness of the 3D Lidar, we decided that testing on the car would give us a better understanding of its performance for tuning and debugging purposes. We chose to evaluate our car in one maze. The maze included islands, dead ends, and tight passageways so as to mimic all possible maze scenarios. In order to collect as much data as possible, we varied the starting position of the car in the maze and measured its performance in all cases."),r.a.createElement("p",null,"We placed our car in 3 positions in the maze. For each position, we measured the distance of the shortest path from the starting position to the goal. We used this distance as a baseline measure for our car. For each of the 3 positions, we ran the maze solver 3 times. We timed the maze solver and we measured (approximately) the total distance traveled by the car. We compared these distances to the baseline of that position."),r.a.createElement("p",null,"Figure 7 indicates the approximate trajectories of the robot compared to the what the ideal trajectory would be given knowledge of the maze in advance. The trajectory of Path 1 is almost identical to that of the ideal trajectory because the robot heads towards open space and never arrives at a junction where it must decide where to go. The trajectory of Path 2 requires some extra distance covered due to the orientation of the robot. The robot is compelled to move forward until occupied space encourages the robot to turn around and head towards the ideal trajectory. Lastly, in Path 3, the robot is faced with a junction and choses to head towards a dead end before heading in the right direction. The robot also takes a longer route to get to the goal point after heading in the right direction. This explains why the standardized distance traveled is significantly greater for Path 3. Overall, standard deviation (as indicated by the error bars) for time traveled (Fig [where the time graph is]) and for distance traveled (Fig [where distance graph is]) is relatively low. This is because the frontier exploration algorithm is deterministic. Each time the robot begins in the same location and orientation, it will explore the same frontiers in the same order."),r.a.createElement("div",{style:{width:"100%",marginBottom:"30px"}},r.a.createElement("img",{src:"/maze_solver/experiments.jpg",style:{width:"100%"}})),r.a.createElement("h4",{id:"4"},"4. LESSONS LEARNED "),r.a.createElement("p",null,"Over the course of this challenge, we learned the importance of prioritizing tasks and having a back-up algorithm."),r.a.createElement("p",null,"At times in the lab, we focused on making the path finding and pure pursuit algorithms more robust while the basic movements of the maze solver did not work as anticipated. For example, we tried to implement dubens curves prior to having a working maze solver. This attempt to improve an already broken algorithm delayed our finding of a minimum viable product, costing the team invaluable time."),r.a.createElement("p",null,"At the same time, we learned that having a simple backup algorithm is essential should the more robust algorithm not be completed. For this task, we realized that a stochastic wall follower algorithm could replace our pure pursuit controller to guide the car through the maze. While a wall follower would most likely take more time to solve a maze, it would at least be able to explore the maze."),r.a.createElement("h4",{id:"5"},"5. POTENTIAL IMPROVEMENTS"),r.a.createElement("p",null,"Our current implementation of a maze solver works well, but it has some issues and there are a lot of improvements we could implement in order to make it more robust."),r.a.createElement("p",null,"The dynamics of the car itself are really important for path planning, but our current implementation does not take these into account. Our current path-planning algorithm for the maze works by using nodes and making paths between these nodes using Dijkstra's algorithm. The nodes are just cells from our down-sampled map, and the cost associated with them is determined by how close the cell is to a wall. By finding the least-cost path through this maze, we try to avoid walls. The issue with this implementation is that it is purely based upon cell cost. The turning radius of the car is never taken into account. As a result, we sometimes publish paths that the car cannot physically follow. We currently rely on our safety controller to get us out of these situations where the car could collide with an object. Using another representation of trajectories such as *dubin* curves would enable us to create smoother paths that would help both avoiding collisions as well as driving with pure pursuit. Having these smoother paths that take into account car dynamics would also allow us to increase the speed at which the car travels. Currently the car explores the car explores the maze slowly because of these uneven paths and to allow the safety controller to kick in soon enough to avoid obstacles."),r.a.createElement("p",null,"Another issue we ran into with our current implementation is that because our current generated paths are only dependent upon low-cost and nearest frontier, we run into the issue where we cannot find a goal point for pure pursuit because our lookahead distance is static. Sometimes the start point of the trajectory is slightly too far from the car and sometimes the end of the trajectory is too close (because the maze environment is concave, the absolute distance between the start and end of a path can be less than one lookahead distance). Our current solution for this problem is generating a new path when we cannot find a goal point for the current one. This is not the most efficient solution, as this means we are no longer exploring the closest frontier and it is computationally expensive to generate new paths, which takes time (as can be seen when the car pauses in the maze to generate new paths). A better solution would be to make a dynamic look-ahead distance and change it accordingly based upon what path is generated and the position of the car with respect to that path."),r.a.createElement("p",null,"Our current lidar is only capable of seeing objects with a minimum distance of 0.5 meters. If an object is closer than 0.5 meters, then the lidar views this as an object that is an infinite distance away. We currently run into issues where our car will re-map a previously discovered wall as not existing because it gets within 0.5 meters of it and thinks that nothing is there. As a result, sometimes paths are generated that go through walls. This happens very rarely, but when it does we rely on wall dilation as well as our safety controller. One proposed solution to this problem is that when we receive a scan of distance infinity from our lidar, we change it to a random number between 0.0 and 0.5 using a uniform distribution. This would solve the issue of deleting/re-mapping pre-existing walls as well as help us avoid generating paths too close to walls. The problem with this is that if a path in the maze is 1 meter or less wide, our car would never be able to traverse it, and would always map that something exists there. Another solution is to find these specific laser scans in the data, and remove them before we send the lidar data to Google Cartographer. This means that no scans that are of a value of infinity would be used, avoiding the remapping of walls. We will test both solutions in future to determine which one is more optimal, but it may vary based upon environment."),r.a.createElement("p",null,"As previously discussed, we want our car to avoid bumping into walls/obstacles at all costs. Currently we dilate our walls using a nearest-neighbors method. This does not always work, as sometimes there are paths being generated that go too close to walls. In the future, we want to dilate our map more to avoid paths that are too close to walls, but we also have to figure out a way to not dilate so much that narrow passages are blocked off. Dilating the map works well for mazes that are more open, but for mazes that are more narrow, it could block off some of the maze."),r.a.createElement("p",null,"Lastly, we want to implement a \u201cdrive to the end\u201d functionality for the car. Our current path planning only looks for the nearest frontier, and creates a path there. However, if we are trying to escape a maze of a given radius, if we discover a point that is outside this given radius, we want to drive there immediately. We did not implement this because it was too computationally expensive to check if a point was discovered outside the radius every time the car wanted to plan a new path."),r.a.createElement("ul",null,r.a.createElement("li",null,"Creating paths that are more geared towards car maneuverability x"),r.a.createElement("li",null,"Dynamic lookahead distance for pure pursuit"),r.a.createElement("li",null,"Not using a velodyne for this maze - Improving upon velodyne remapping because a space is within 0.5m"),r.a.createElement("li",null,"If it discovers an open space far enough away, drive there immediately"),r.a.createElement("li",null,"Computationally expensive"),r.a.createElement("li",null,"Dilating walls more to avoid wall collisions, improving safety controller"),r.a.createElement("li",null,"Faster maze movement (but cartographer wacks out)"))))}}]),a}(n.Component)),E=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{style:{textAlign:"justify"}},r.a.createElement("h2",null," Wall3D: Maze Solving Challenge "),r.a.createElement("p",{align:"middle"},r.a.createElement("i",null,"Team 9: Talia Pelts, Kevin Carlson, Jonathan Samayoa, Susan Ni, Vlad Seremet")),r.a.createElement(w.a,{defaultActiveKey:"overview",style:{fontWeight:"bold"},id:"uncontrolled-tab-example"},r.a.createElement(m.a,{eventKey:"overview",title:"General Overview"},r.a.createElement(b,null)),r.a.createElement(m.a,{eventKey:"report",title:"Technical Report"},r.a.createElement(v,null))))}}]),a}(n.Component),y=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{style:{textAlign:"justify"}},r.a.createElement("h2",null,"HarvesR: using reinforcement learning for fruit picking"),r.a.createElement("h5",null," Table of Contents:"),r.a.createElement("ol",null,r.a.createElement("li",null,r.a.createElement("a",{href:"#1"},"Introduction")),r.a.createElement("li",null,r.a.createElement("a",{href:"#2"},"Robot Architectore and the Unity Simulation Environment"),r.a.createElement("ol",{type:"a"},r.a.createElement("li",null,r.a.createElement("a",{href:"#2a"},"Robot Architecture")),r.a.createElement("li",null,r.a.createElement("a",{href:"#2b"},"The Simulation Environment")))),r.a.createElement("li",null,r.a.createElement("a",{href:"#3"},"Internal Model Representation and Planning"),r.a.createElement("ol",{type:"a"},r.a.createElement("li",null,r.a.createElement("a",{href:"#3a"},"Overview")),r.a.createElement("li",null,r.a.createElement("a",{href:"#3b"},"Encoder-Decoder Architecture")),r.a.createElement("li",null,r.a.createElement("a",{href:"#3c"},"Forward Model")),r.a.createElement("li",null,r.a.createElement("a",{href:"#3d"},"Planning")))),r.a.createElement("li",null,r.a.createElement("a",{href:"#4"},"Control")),r.a.createElement("li",null,r.a.createElement("a",{href:"#5"},"Training"),r.a.createElement("ol",{type:"a"},r.a.createElement("li",null,r.a.createElement("a",{href:"#5a"},"Pre-Training")),r.a.createElement("li",null,r.a.createElement("a",{href:"#5b"},"Unity and the training setup")))),r.a.createElement("li",null,r.a.createElement("a",{href:"#6"},"Conclusion"))),r.a.createElement("h4",{id:"1"},"1. Introduction "),r.a.createElement("div",null,r.a.createElement("div",{id:"fig1",className:"halfscreen"},r.a.createElement("img",{src:"/harvestr/fig1.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 1"),"\xa0 Averages and range of reported quantitative performance indicators: localization success, detachment success, harvest success, fruit damage, and peduncle damage. N represents the number of distinct projects. (Bac et al.)")),r.a.createElement("p",null,'Because fruit harvesting is classified as a low skill job, pundits routinely put it in the category of jobs on the verge of automation. However, in accordance to Moravec\u2019s paradox, in actuality, fruit picking is much harder to automate than many jobs that are regarded as \u201chigh skill\u201d, such as accounting, radiography, stock trading etc. In Morave\u2019s words: "it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility". This is in part due to the high variability in the environment, and high adaptability required to conditions such as fruit shape/distance/color, lighting, occlusion etc. A review of the state of the art in fruit harvesting concluded that the performance of harvesting robots has been stagnant for the past 30 years [1]. The same review found that the average harvest success was 66% (Fig. 1) and the average cycle time was 33s. It is worth mentioning that this data is skewed by easily harvestable crops such as kiwis, apples, roses etc. For crops such as peppers, tomatoes and strawberries the performance is even worse.'),r.a.createElement("p",null,"So far, harvesting robots have used classical algorithms for planning and execution. As such, it is very hard to anticipate and hard code all the conditions a harvesting robot might find itself in."),r.a.createElement("p",null,"This paper proposes a different approach \u2013 one based on reinforcement learning. Classical planning is still used for moving from one fruit to another, but once the gripper is in proximity of a fruit, a neural network trained in simulation is used to generate a control sequence."),r.a.createElement("p",null,"Section 1 describes the architecture of the robot and the simulation environment used for simulating the plant dynamics, and the robot-plant interactions. Section 2 details the internal model used for planning, how this model is acquired in simulation, how this model transfers from simulation to the real world, and how this model is acquired in the real world. Section 3 discusses the control algorithm. Section 4 discusses the training methodology.")),r.a.createElement("h4",{id:"2"},"2. Robot architecture and the Unity simulation environment"),r.a.createElement("h5",{id:"2a"},"2.a Robot Architecture"),r.a.createElement("div",null,r.a.createElement("div",{id:"fig2",className:"halfscreen"},r.a.createElement("img",{src:"/harvestr/fig2.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 2"),"\xa0 4 Degree of freedom (DOF) fruit-picking robot. Similar systems are used in many harvesting robots, as it strikes a good balance between simplicity and versatility")),r.a.createElement("p",null,"Although the HarvestR architecture makes few assumptions about the architecture of the robot, for demonstration and testing a 4DOF robot is used (Fig.2). The robot can move across a raw of crops, up and down to harvest plants at different heights, it can rotate in order to navigate between stems, leaves and other obstacles and adapt to the position of the target fruit, and it can extend its arm. Similar designs are used in most harvesting robots [1], as they have very simple kinematics."),r.a.createElement("p",null,"The action space is a 5-element vector of numbers between [-1,1] that specify the motion of each DOF (plus gripper) in the interval [-maxSpeed*dt, maxSpeed*dt], where maxSpeed is a parameter defined for each DOF. The observation space (the data received from the simulation after each timestep) consists of 1 RGB image, 1 depth map, 1 semantic map (each pixel has a label), 256x256 pixels each, and the 32x32x32 semantic occupancy grid. The sensors are illustrated in Fig.3.")),r.a.createElement("h5",{id:"2b"},"2.b The Simulation Environment"),r.a.createElement("p",null,"One of the main hurdles in the real-world application of reinforcement learning algorithms is sample inefficiency. It requires millions of training examples for a reinforcement learning algorithm to start performing well, which is unrealistic to acquire in a real world environment. Fortunately, the computing power available today allows us to simulate complex environments with complicated dynamics. Moreover, with the recent advancements in reinforcement learning, many simulation environments provide APIsthat make it easier to define trainable agents and extract useful observation data (i.e. MuJoCo, PyBullet, Gazebo, MATLAB Simulink, Unity)."),r.a.createElement("p",null,"For this particular application, Unity was chosen for the following reasons:",r.a.createElement("ul",null,r.a.createElement("li",null,"Provides many off the shelf packages which makes development easier. For example, Obi is a package for simulating ropes and rods \u2013 very useful for simulating plant stems. Unity-voxel is a package that transforms meshes into voxelgrids \u2013 useful for creating 3D occupancy grids."),r.a.createElement("li",null,"ML-Agents Python API. An interface for defining agents in the simulation environment, sending actions and receiving observations and rewards."),r.a.createElement("li",null,"Very easy to train multiple agents in parallel. By simply copying the training environment in the same scene, many clones of the same agent can be trained simultaneously. Moreover, one can launch many simulations across multiple machines, and control them from a central client using remote requests (for example using (RPyC)."),r.a.createElement("li",null,"Has a large variety of predefined simulated sensors (i.e. ray vision, RGBD cameras \u2013 most important for this application, Accelerometers, Pressure Sensors etc.). Moreover, one can define their own sensors, such as a sensor for retrieving an occupancy grid."),r.a.createElement("li",null,"Has a C# scripting API that allows to easily create many randomized environments based on prefabs \u2013 important for making the algorithm adaptable and transferable to the real world."),r.a.createElement("li",null,"Allows for the creation of manual controllers \u2013 important for imitation learning."))),r.a.createElement("h4",{id:"3"},"3. Internal model representation and planning"),r.a.createElement("div",null,r.a.createElement("div",null,r.a.createElement("div",{id:"fig3",className:"onethirdscreen"},"a)",r.a.createElement("img",{src:"/harvestr/fig3a.png",style:{width:"100%"}}),"b)",r.a.createElement("img",{src:"/harvestr/fig3b.png",style:{width:"100%"}}),"c)",r.a.createElement("img",{src:"/harvestr/fig3c.png",style:{width:"100%"}}),"d)",r.a.createElement("img",{src:"/harvestr/fig3d.png",style:{width:"100%"}}),"e)",r.a.createElement("img",{src:"/harvestr/fig3e.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 3"),"\xa0 a) the transparent cube is a grid of colliders that constructs the occupancy grid. b) camera view c) Depth Image d) Semantic labeling e) Occupancy Grid"))),r.a.createElement("h5",{id:"3a"},"3.a Overview"),r.a.createElement("p",null,"The HarvestR architecture assumes one camera mounted above the gripper, so that it has a view of the area captured by the occupancy grid. When the robot is deployed in a physical environment, the camera data is augmented before it is fed into the policy network and forward model for planning. Mapping images directly to actions is possible, but very sample inefficient. Using only a convolutional neural network to map images to actions, the network would have to learn a 3D representation of the environment implicitly, since the actions are in 3D space. Instead, a better solution is using semi-supervised representation learning with a variational encoder to map images into a latent space that captures the relevant spatial information (section 2.2). This latent vector is then used as input to a forward model for prediction and planning (section 2.3)."),r.a.createElement("p",null,"This latent space can easily be decoded into a semantic occupancy grid \u2013 each cell has an integer label (i.e. 0 \u2013 empty, 1 - leaf, 2 \u2013 stem, 3 \u2013 fruit that is not ripe, 4 \u2013 fruit that is ripe)."),r.a.createElement("p",null,"This approach has many benefits:",r.a.createElement("ul",null,r.a.createElement("li",null,"The inference of the model from camera data can be separated from planning, which allows the control system to adapt across different environments."),r.a.createElement("li",null,"On a structural and semantic level, many plants are very similar (most have stems, leaves, peduncles and fruits). Using a semantically labelled occupancy grid allows the robot to capture this structure."),r.a.createElement("li",null,"It is easy to build a forward model, policy network and value function based on the latent representation."),r.a.createElement("li",null,"Easy to interpret for humans. Feeding the latent representation into the decoder essentially allows us to visualize in 3D what the robot is \u201cseeing\u201d.")))),r.a.createElement("div",{style:{display:"inline-block"}},r.a.createElement("h5",{id:"3b"},"3.b Econder-Decoder Architecture"),r.a.createElement("p",null,"First, a disentangled variational encoder [5] is implemented - very similar to the one used in [2], except unlike an autoencoder, the input and output are different. The purpose of the encoder is to create an embedding \u2013 a vector that contains enough information that it can be fed into the decoder, and the original occupancy grid could be reconstructed. This encoder takes as input a 256x256 depth map and a 256x256 semantic label map and outputs a 32x32x32 semantic occupancy grid (Fig. 4). All this data can be obtained from simulation so the training data is virtually unlimited."),r.a.createElement("div",{id:"fig5",className:"halfscreen"},r.a.createElement("img",{src:"/harvestr/fig5.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 5 "),"\xa0 Convolutional network for semantic labeling [3].")),r.a.createElement("p",null,"The loss function used to train the encoder is:"),r.a.createElement("p",null,"FORMULA"),r.a.createElement("p",null,"where \uf071 represents the parameters of the decoder, \uf066 - parameters of the encoder, x - input, y - output, z - latent vector. The second term in the loss function is a penalty for the latent vector departing from an isometric Gaussian distribution, which essentially forces the latent vector to retain only the elements that are essential. This is important, since this vector is used as input to the forward model, policy network and value function."),r.a.createElement("h5",{id:"3c"},"3.c Forward Model"),r.a.createElement("p",null,"The function of the forward model is essentially to predict the future based on past experience. Formally, a forward model is a function M parametrized by \uf071 , such that"),r.a.createElement("p",null,"FORMULA"),r.a.createElement("p",null,"t s is a concatenation of the latent vector from the encoder (128 elements) with the joint positions of the robot (5 elements). t a is a 5 element vector with actions for each joint. The input to the forward model therefore is a 138 element vector. t 1 s \uf02b is a concatenation between the latent vector and the joint positions at t \uf02b1. The forward model is implemented as a feed-forward neural network."),r.a.createElement("h5",{id:"3d"},"3.d Planning"),r.a.createElement("p",null,"The joint positions are appended to the latent vector and the resulting vector is fed into the policy network and 5 actions are sampled. These actions are concatenated with t s and fed into the forward model to generate 5 potential t 1 s \uf02b . This process is iterated 5 times until 3125 potential t 5 s \uf02b states are acquired. The value function is used to select the top 5 states. The rest are discarded. This process is iterated 10 times until t 50 s \uf02b is reached. The value function is used to pick the top state and the trajectory \uf028s a s a s a a s t t t t t t t t , , , , , ,......., , \uf02b \uf02b \uf02b \uf02b \uf02b \uf02b 1 1 2 2 49 50 \uf029 is given to the controller to follow. The robot follows the given trajectory until exp | z | ected observed t t \uf02d \uf03e z \uf065 or the trajectory has been completed. t z is the latent vector at time t."),r.a.createElement("div",{id:"fig4"},r.a.createElement("img",{src:"/harvestr/fig4.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 5 "),"\xa0 Encoder-decoder architecture and planning based on latent representation"))),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("h4",{id:"4"},"4. Control"),r.a.createElement("p",null,"At the highest level, the control loop is essentially a state machine (Fig. 6) The states colored in blue use policies obtained through reinforcement learning for control. These states have their own policy network, reward functions, and value functions associated with them.",r.a.createElement("ul",null,r.a.createElement("li",null,"\u201cFind Fruit\u201d uses semantic labeling on camera images to find the nearest ripe fruit. Once a fruit is found, control is passed to \u201cGet Close\u201d."),r.a.createElement("li",null,"\u201cGet Close\u201d simply moves the robot towards the target fruit, once the target fruit is in the range of the occupancy grid, control is passed to \u201cmove and cut\u201d."),r.a.createElement("li",null,"\u201cMove and cut\u201d uses a trained policy, value function and the forward model to generate trajectories towards accomplishing its goal \u2013 getting the gripper around the peduncle of the target fruit, cutting it and holding it. Once this is accomplished or interrupted, control is passed to \u201ccheck status\u201d."),r.a.createElement("li",null,"\u201cCheck status\u201d has no control over the joints. It simply assesses the situation and moves to the appropriate state. If all is good and the fruit is held by the gripper, control is passed to \u201cMove and drop\u201d."),r.a.createElement("li",null,"\u201cMove and drop\u201d uses a policy network, a value function and a forward model to generate trajectories towards accomplishing its goal: getting the fruit in the basket. If the fruit reaches the basket, the control loop starts over, if not, control is passed to \u201ccheck status\u201d."))),r.a.createElement("div",{id:"fig6"},r.a.createElement("img",{src:"/harvestr/fig6.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 6 "),"\xa0 Control loop")),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("h4",{id:"5"},"5. Training"),r.a.createElement("h5",{id:"5a"},"5.a Pretraining"),r.a.createElement("p",null,"Since training the policy and value function is dependent on the encoder, the segmentation convolutional net and the forward model, these need to be trained first. For that purpose, many environments with different numbers and placements of plants, different lighting conditions and textures are defined. The robot is programmed to move around somewhat randomly. All the observations and actions are stored in a buffer and the parameters of the encoder, forward model and segmentation convolutional net are updated offline based on this data."),r.a.createElement("p",null,"Next, the policy and value function of \u201cmove and cut\u201d and \u201cmove and drop\u201d are initialized."),r.a.createElement("p",null,"For \u201cmove and cut\u201d, simple environments (with no overlapping stems) are generated, and a path planning program is written to guide the gripper towards the peduncle and cut it, while the observations, actions and rewards are recorded and used to update the policy network and value function. This is essentially done to give the robot a \u201chead start\u201d and to help it \u201cunderstand\u201d that, for example, the gripper being around the peduncle of the target fruit is a highly desirable position, while a collision with a stem or a leaf is undesirable."),r.a.createElement("p",null,"Similarly, for \u201cmove and drop\u201d, environments are generated programmatically, the robot is initialized with a fruit in its gripper and a program is written to guide the robot towards the basket and drop the fruit, while the observations and actions are stored into a buffer and used to update the policy network, value function and forward model."),r.a.createElement("p",null,"After the initialization is done, it\u2019s time to move to actual training. Environments are generated procedurally with randomization across multiple parameters such as the starting position of the robot, number, positions, sizes of plants, lighting conditions, textures etc. Trajectories and camera data are saved for every time step and later used for updating the encoder, forward model and the segmentation convolutional neural net. After these updates have been made, the data is discarded."),r.a.createElement("div",{id:"fig7"},r.a.createElement("img",{src:"/harvestr/fig8.png",style:{width:"100%"}}),r.a.createElement("div",null,r.a.createElement("b",null,"Fig. 7 "),"\xa0 Training multiple environments in parallel. Each platform is a separate training environment.")),r.a.createElement("br",null),r.a.createElement("h5",{id:"5b"}," 5.b A few words about Unity and the training setup: "),r.a.createElement("p",null,"For the Unity ML-agents python API, a gym-like wrapper, UnityEnvWrapper, was implemented in order to make it easier to interact with agents. In Unity it is possible to create as many training environments as you want just by instantiating a prefab environment in the same scene multiple times (Fig. 7), and when env.step() is called, it returns a list of the agents that are waiting for an action, together with their observations and rewards. Unity ML-agents is a package that facilitates the use of Unity as a Reinforcement Learning framework. It provides an Agent interface, which allows the user to define the reward function, the observations collected at each step, the action space, how the robot responds to each action, and other useful features."),r.a.createElement("p",null,"A big hurdle was finding a good way to simulate the dynamics of plants, in particular the movement of a plant stem when it is pushed around by the robot."),r.a.createElement("p",null,"Experiments were performed with various designs based on chains of rigid bodies connected with hinge joints, spring joints, custom joints etc. but to no avail. They all resulted in either very unrealistic, stiff motion, or jittering and instability on interaction, especially as the number of objects increased. Thankfully there is a package called Obi, which defines ropes and rods with adjustable rigidity, springiness, bending, stretching, tearing etc. With Obi rods it was possible to create a plant that behaves realistically in simulation."),r.a.createElement("p",null,"Another package used in this implementation is ImageSynthesis. This package provides a very simple API for retrieving camera data together with depth maps and semantic labels."),r.a.createElement("h4",{id:"6"}," 6. Conclusion"),r.a.createElement("p",null,"Despite the immense progress in Reinforcement Learning and Machine Learning in general, a seemingly simple task such as fruit picking still remains elusive. This paper discussed why that is the case and proposed HarvestR \u2013 a control system for harvesting robots based on deep reinforcement learning. At the heart of HarvestR is a disentangled encoder that learns an embedding that captures the relevant semantic data and spatial structure of a scene in a form that is easy to feed into a state of the art RL algorithm such as PPO. The power of PPO is increased by the use of a forward model for look-ahead ."),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("h4",null,"Bibliography"),"[1] Bac, C. Wouter, Eldert J. van Henten, Jochen Hemming, and Yael Edan. \u201cHarvesting Robots for High-Value Crops: State-of-the-Art Review and Challenges Ahead: Harvesting Robots for High-Value Crops: State-of-the-Art Review and Challenges Ahead.\u201d Journal of Field Robotics 31, no. 6 (November 2014): 888\u2013911. https://doi.org/10.1002/rob.21525.",r.a.createElement("br",null),"[2] Sch\xf6nberger, Johannes L., Marc Pollefeys, Andreas Geiger, and Torsten Sattler. \u201cSemantic Visual Localization.\u201d ArXiv:1712.05773 [Cs], April 16, 2018. http://arxiv.org/abs/1712.05773.",r.a.createElement("br",null),"[3] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. \u201cFully Convolutional Networks for Semantic Segmentation.\u201d ArXiv:1411.4038 [Cs], March 8, 2015. http://arxiv.org/abs/1411.4038.",r.a.createElement("br",null),"[4] sagieppel. \u201cSemantic Segmentation with Fully Convolutional Neural Network (FCN) Pytorch Implementation.\u201d github, n.d. https://github.com/sagieppel/Fully-convolutional-neural-network-FCNfor-semantic-segmentation-with-pytorch.",r.a.createElement("br",null),"[5] Higgins, Irina, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed, and Alexander Lerchner. \u201cEarly Visual Concept Learning with Unsupervised Deep Learning.\u201d ArXiv:1606.05579 [Cs, q-Bio, Stat], September 20, 2016. http://arxiv.org/abs/1606.05579.")}}]),a}(n.Component),k=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement("h2",null,"Platformer: a fun 2D game"))}}]),a}(n.Component),x=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement(m.a.Container,{id:"left-tabs-example",defaultActiveKey:"first"},r.a.createElement(d.a,null,r.a.createElement(u.a,{lg:3},r.a.createElement(p.a,{variant:"tabs",className:"flex-column"},r.a.createElement(p.a.Item,null,r.a.createElement(g.b,{to:"/projects/",className:"navlink navlink-left",activeStyle:{border:"1px solid black",borderRadius:"10px"},exact:!0},"Maze Solver")),r.a.createElement(p.a.Item,null,r.a.createElement(g.b,{to:"/projects/harvestr",className:"navlink navlink-left",activeStyle:{border:"1px solid black",borderRadius:"10px"},exact:!0},"HarvestR")),r.a.createElement(p.a.Item,null,r.a.createElement(g.b,{to:"/projects/platformer",className:"navlink navlink-left",activeStyle:{border:"1px solid black",borderRadius:"10px"},exact:!0},"Platformer")))),r.a.createElement(u.a,{lg:9},r.a.createElement(m.a.Content,null,r.a.createElement(f.a,{path:"/projects/",component:E,exact:!0}),r.a.createElement(f.a,{path:"/projects/harvestr",component:y,exact:!0}),r.a.createElement(f.a,{path:"/projects/platformer",component:k,exact:!0})))))}}]),a}(n.Component),T=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{style:{verticalAlign:"center"}},r.a.createElement("table",{id:"t",width:"100%",style:{textAlign:"left",verticalAlign:"center",margin:"20px"}},r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("h4",null,"Education"))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,"Massachusetts Institute of Technology "),r.a.createElement("br",null),r.a.createElement("div",{className:"subtitle"},"Relevant Coursework: \xa0",r.a.createElement("a",{href:"https://mc.ai/6-141-robotics-science-systems-a-review/",target:"_blank"},"Robotics: Science and Systems,"),"\xa0",r.a.createElement("a",{href:"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/",target:"_blank"},"Performance Engineering of Software Systems*,"),"\xa0",r.a.createElement("a",{href:"http://web.mit.edu/6.033/www/general.shtml",target:"_blank"},"Computer Systems Engineering*,"),"\xa0",r.a.createElement("a",{href:"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-005-software-construction-spring-2016/",target:"_blank"},"Elements of Software Construction*,"),"\xa0",r.a.createElement("a",{href:"http://courses.csail.mit.edu/6.036/",target:"_blank"},"Machine Learning,"),"\xa0",r.a.createElement("a",{href:"https://py.mit.edu/fall20",target:"_blank"},"Fundamentals of Programming,"),"\xa0",r.a.createElement("a",{href:"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/",target:"_blank"},"Introduction to Algorithms,"),"\xa0",r.a.createElement("a",{href:"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/",target:"_blank"},"Introduction to Data Science,"),"\xa0",r.a.createElement("a",{href:"https://courses.csail.mit.edu/6.042/spring18/classinfo.shtml",target:"_blank"},"Mathematics for Computer Science,"),"\xa0",r.a.createElement("a",{href:"https://www.eecs.mit.edu/academics-admissions/academic-information/subject-updates-spring-2020/6884",target:"_blank"},"Computational Sensorimotor Learning*"),"\xa0",r.a.createElement("a",{href:"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/",target:"_blank"},"Computation Structures*,"),r.a.createElement("br",null),"* - project based class")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Cambridge, MA ",r.a.createElement("br",null),"2015-2020")),r.a.createElement("tr",null,r.a.createElement("td",{style:{paddingTop:"20px"}},r.a.createElement("h4",null,"Experience"))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,r.a.createElement("a",{href:"http://darbelofflab.mit.edu/",target:"_blank"},"MIT D\u2019Arbeloff Laboratory")),r.a.createElement("div",{className:"subtitle"},"Undergraduate Research Assistant \u2013 UROP in robotics")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Cambridge, MA ",r.a.createElement("br",null),"Feb.\u201819 \u2013 May \u201819")),r.a.createElement("tr",null,r.a.createElement("ul",null,r.a.createElement("li",null," Programmed a UR robotic arm as part of \xa0",r.a.createElement("a",{href:"http://www.mit.edu/~nselby/teachbot.html",target:"_blank"},"TeachBot - an interactive apprenticeship program.")),r.a.createElement("li",null," Worked with ROS, MoveIt, ReactJs, ExpressJs."))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,"Amazon Web Services"),r.a.createElement("div",{className:"subtitle"},"Software Engineering Intern")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Berlin, Germany",r.a.createElement("br",null),"Jun.-Aug. 2017")),r.a.createElement("tr",null,r.a.createElement("ul",null,r.a.createElement("li",null," Improved the refund approval workflow by building a microservice that helps tech assistants review purchase history."),r.a.createElement("li",null," Technologies used: AngularJS, ExpressJS, Java, Other internal AWS tools"))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,r.a.createElement("a",{href:"http://acl.mit.edu/",target:"_blank"},"MIT Aerospace Controls Lab")),r.a.createElement("div",{className:"subtitle"},"Undergraduate Research Assistant - UROP in Machine Learning/Robotics")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Cambridge, MA",r.a.createElement("br",null),"Feb. \u2013 May. \u201817")),r.a.createElement("tr",null,r.a.createElement("ul",null,r.a.createElement("li",null," Created a pipeline for automated acquisition, and processing of image datasets."),r.a.createElement("li",null," Designed and tested algorithms for \xa0",r.a.createElement("a",{href:"http://acl.mit.edu/projects/socially-acceptable-navigation",target:"_blank"},"navigating through crowds while avoiding collisions.")))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,"MIT"),r.a.createElement("div",{className:"subtitle"},"Teacher Assistant for  \xa0",r.a.createElement("a",{href:"https://py.mit.edu/fall20",target:"_blank"},"6.009 (fundamentals of programming using Python)"))),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Cambridge, MA",r.a.createElement("br",null),"Sep. \u201817 \u2013 May \u201818")),r.a.createElement("tr",null,r.a.createElement("ul",null,r.a.createElement("li",null,"Helped teachers with assignments and student assistance."),r.a.createElement("li",null,"Explained concepts such as iteration, recursion, data structures in python, debugging techniques, writing efficient code, \u201cthe pythonic way\u201d etc."))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,r.a.createElement("a",{href:"https://oneshore.com/",target:"_blank"},"OneShore Energy GmbH")),r.a.createElement("div",{className:"subtitle"},"Software Engineering Intern")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Berlin, Germany",r.a.createElement("br",null),"Summer 2016")),r.a.createElement("tr",null,r.a.createElement("ul",null,r.a.createElement("li",null,"Improved the data visualization process by developing a dashboard using Django on the backend and HTML/CSS/JS (jQuery, highcharts) on the frontend."),r.a.createElement("li",null,"Debugged and improved the efficiency of a simulation tool that predicts the performance of the electrical system (replaced and improved some algorithms and data structures used in the simulation and replaced some Python code with C++)."))),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("h4",null,"Leadership"))),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,r.a.createElement("a",{href:"https://gelp.mit.edu/",target:"_blank"},"Gordon Engineering Leadership Program")),r.a.createElement("div",{className:"subtitle"},"Trainee")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Cambridge, MA ",r.a.createElement("br",null),"2017")),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,r.a.createElement("a",{href:"https://waveweekmoldova.wordpress.com/",target:"_blank"},"Wave Week Moldova \u2013 a leadership program for students")),r.a.createElement("div",{className:"subtitle"},"Staff member and coach")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Chisinau, Moldova",r.a.createElement("br",null),"2012-2015")),r.a.createElement("tr",null,r.a.createElement("td",{width:"80%"},r.a.createElement("b",null,"English Debate Club at the \xa0",r.a.createElement("a",{href:"https://americahouse.md/",target:"_blank"},"American Resource Center")),r.a.createElement("div",{className:"subtitle"},"Moderator and Coach")),r.a.createElement("td",{width:"20%",style:{textAlign:"center"}},"Chisinau, Moldova",r.a.createElement("br",null),"2014-2015")))),r.a.createElement("table",{id:"t",width:"100%",style:{textAlign:"left",verticalAlign:"center",margin:"20px"}},r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("h4",null,"Skills"))),r.a.createElement("tr",null,r.a.createElement("td",{width:"20%"}," ",r.a.createElement("b",null," Robotics: ")," "),r.a.createElement("td",{width:"80%"},"ROS, Gazebo, Unity ML-Agents, MuJoCo, Pybullet, Fusion 360, Solidworks, Blender")),r.a.createElement("tr",null,r.a.createElement("td",{width:"20%"}," ",r.a.createElement("b",null," AI/ML: ")," "),r.a.createElement("td",{width:"80%"},"Pytorch, reverb, CNNs, Autoencoders, Object Instance/Semantic segmentation")),r.a.createElement("tr",null,r.a.createElement("td",{width:"20%"}," ",r.a.createElement("b",null," WebDev: ")," "),r.a.createElement("td",{width:"80%"},"Django, JQuery, ReactJS, ExpressJS, HTML/CSS/JS")),r.a.createElement("tr",null,r.a.createElement("td",{width:"20%"}," ",r.a.createElement("b",null," Other: ")," "),r.a.createElement("td",{width:"80%"},"Linux, Docker, Arduino, Adobe Photoshop, git, vim, web scraping, violin, piano, drums")),r.a.createElement("tr",null,r.a.createElement("td",{width:"20%"}," ",r.a.createElement("b",null," Languages: ")," "),r.a.createElement("td",{width:"80%"},"Romanian (native), Russian (fluent), English (fluent), German (beginner)"))))}}]),a}(n.Component),A=function(e){Object(c.a)(a,e);var t=Object(h.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{style:{background:"url('/background.png')",backgroundSize:"contain",backgroundRepeat:"no-repeat",width:"100%",height:"1000px",color:"white"}},r.a.createElement("div",{className:"introTextBox"},r.a.createElement("p",null,"Hi, my name is Vlad and I'm an MIT student specializing in robotics and artificial intelligence. My main interest is in programming autonomous robots that can do useful physical tasks, such as picking irregular objects, harvesting and assembly. To that end, I study Control Theory, Reinforcemnt Learning, Immitation Learning, Computer Vision, Planning, and Simulation.",r.a.createElement("br",null),"If you are working on similar things, feel free to contact me: seremetv AT mit.edu")))}}]),a}(n.Component);a(59);var j=function(){return r.a.createElement(g.a,null,r.a.createElement("div",{className:"App main-container"},r.a.createElement("div",{className:"navbar"},r.a.createElement("div",{className:"left"},r.a.createElement(g.b,{to:"/",exact:!0,style:{color:"black",textDecoration:"none"}},r.a.createElement("h2",null,"Vlad Seremet"))),r.a.createElement("div",{className:"right",style:{fontSize:"20"}},r.a.createElement(g.b,{to:"/resume",className:"navlink",activeStyle:{borderBottom:"1px solid black"},exact:!0},"Resume"),r.a.createElement(g.b,{to:"/projects",className:"navlink",activeStyle:{borderBottom:"solid 1px black"}},"Projects"),r.a.createElement(g.b,{to:"/blog",className:"navlink",activeStyle:{borderBottom:"solid 1px black"}},"Blog"))),r.a.createElement(f.a,{path:"/",component:A,exact:!0}),r.a.createElement(f.a,{path:"/resume",component:T,exact:!0}),r.a.createElement(f.a,{path:"/projects",component:x})))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));l.a.render(r.a.createElement(r.a.StrictMode,null,r.a.createElement(j,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[43,1,2]]]);
//# sourceMappingURL=main.6f385992.chunk.js.map