{"version":3,"sources":["components/mazeSolverOverview.jsx","components/mazeSolverReport.jsx","components/mazeSolverProject.jsx","components/harvestRProject.jsx","components/platformerProject.jsx","components/projects.jsx","components/resume.jsx","components/greeting.jsx","App.js","serviceWorker.js","index.js"],"names":["MazeSolverOverview","style","width","float","src","className","frameborder","allow","allowfullscreen","height","mozallowfullscreen","webkitallowfullscreen","position","display","Component","MazeSolverReport","href","type","id","marginBottom","MazeSolverProject","textAlign","align","Tabs","defaultActiveKey","fontWeight","Tab","eventKey","title","HarvestRProject","PlatformerProject","Projects","Container","Row","Col","lg","Nav","variant","Item","to","activeStyle","border","borderRadius","exact","Content","path","component","Resume","verticalAlign","margin","target","paddingTop","Greeting","background","backgroundSize","backgroundRepeat","color","App","basename","process","textDecoration","fontSize","borderBottom","Boolean","window","location","hostname","match","ReactDOM","render","StrictMode","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message"],"mappings":"mVAKqBA,E,uKAEjB,OACI,6BAEA,yBAAKC,MACH,CACEC,MAAM,MACNC,MAAO,SAERC,IAAK,4BAER,iQAIA,4JAGA,oJAEA,mHAIA,yBAAKC,UAAU,iBACb,4BAAQA,UAAU,cACVD,IAAI,4CACJE,YAAY,IACZC,MAAM,0EACNC,iBAAe,KAIzB,sNAIA,yBAAKH,UAAU,iBACb,4BAAQA,UAAU,cACVD,IAAI,4KACJE,YAAY,IACZJ,MAAM,MACNO,OAAO,MACPD,gBAAgB,OAChBE,mBAAmB,OACnBC,sBAAsB,UAIhC,qLAGA,yBAAKN,UAAU,iBACb,4BAAQA,UAAU,cACVD,IAAI,4CACJE,YAAY,IACZC,MAAM,0EACNC,iBAAe,KAIzB,8IAIA,yBAAKP,MAAO,CAACC,MAAM,OAAQU,SAAS,aAClC,6BACE,yBAAKX,MAAO,CAACC,MAAM,MAAOW,QAAQ,iBAAlC,uBACA,yBAAKZ,MAAO,CAACC,MAAM,MAAOW,QAAQ,iBAAlC,aAGF,6BACE,yBAAKZ,MAAO,CAACE,MAAM,OAAQD,MAAM,QAC/B,4BAAQA,MAAM,OAAOE,IAAI,4CAA4CE,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,KAEtL,yBAAKP,MAAO,CAACE,MAAM,QAASD,MAAM,QAChC,4BAAQA,MAAM,OAAOE,IAAI,4CAA4CE,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,OAK1L,0CACA,qhBAKA,yBAAKP,MAAO,CAACC,MAAM,QAASE,IAAK,+B,GAxFOU,aCC3BC,G,6KAEjB,OACE,6BAEE,kDAGI,4BACA,4BAAI,uBAAGC,KAAK,MAAR,6BACJ,4BAAI,uBAAGA,KAAK,MAAR,qBACA,wBAAIC,KAAK,KACL,4BAAI,uBAAGD,KAAK,OAAR,yBACJ,4BAAI,uBAAGA,KAAK,OAAR,kBACJ,4BAAI,uBAAGA,KAAK,OAAR,0BAIZ,4BAAI,uBAAGA,KAAK,MAAR,4BACJ,4BAAI,uBAAGA,KAAK,MAAR,oBACJ,4BAAI,uBAAGA,KAAK,MAAR,4BAIR,6BACI,wBAAIE,GAAG,KAAP,iCAEA,6BACA,yBAAKb,UAAU,aAAaD,IAAK,kCACjC,ojCAMA,wBAAIc,GAAG,KAAP,yBACA,4wBAIA,wBAAIA,GAAG,MAAP,4BACQ,iDACA,6BACI,6BACI,yBAAKb,UAAU,cACX,yBAAKJ,MAAO,CAACC,MAAM,QAASE,IAAK,gCACjC,yBAAKH,MAAO,CAACC,MAAM,QAASE,IAAK,mCAGzC,ysBAKZ,4BACI,4BACI,8FACA,0ZAIJ,4BACI,mDACA,mbAIJ,4BACI,uEACA,gcAMR,wBAAIc,GAAG,MAAP,qBACA,6BACI,6BACI,yBAAKb,UAAU,cACX,yBAAKJ,MAAO,CAACC,MAAM,QAASE,IAAK,gCAGzC,qSAGA,uNAGA,8VAKJ,wBAAIc,GAAG,MAAP,2BACA,myBAGA,yBAAKjB,MAAO,CAACC,MAAM,QAASE,IAAK,kCACjC,wrBAGA,6BACI,6BACI,yBAAKC,UAAU,cACX,yBAAKJ,MAAO,CAACC,MAAM,QAASE,IAAK,0CAGzC,mvBAMJ,wBAAIc,GAAG,KAAP,8BACA,onBAGA,0cAGA,kxCAIA,yBAAKjB,MAAO,CAACC,MAAM,OAAQiB,aAAa,SACpC,yBAAKf,IAAK,+BAAgCH,MAAO,CAACC,MAAM,WAG5D,wBAAIgB,GAAG,KAAP,uBACA,iJAGA,0aAGA,0aAIA,wBAAIA,GAAG,KAAP,6BACA,qMAGA,uzCAGA,kmCAGA,o5CAGA,6lBAGA,qhBAGA,4BACI,iGACA,2EACA,qIACA,sGACA,yDACA,yGACA,wF,GArK8BJ,cCCzBM,E,uKAEjB,OACE,yBAAKnB,MAAO,CACVoB,UAAW,YAEX,gEACA,uBAAGC,MAAM,UAAS,4GAElB,kBAACC,EAAA,EAAD,CAAMC,iBAAiB,WAAWvB,MAAO,CAACwB,WAAW,QAASP,GAAG,4BAC/D,kBAACQ,EAAA,EAAD,CAAKC,SAAS,WAAWC,MAAM,oBAC7B,kBAAC,EAAD,OAEF,kBAACF,EAAA,EAAD,CAAKC,SAAS,SAASC,MAAM,oBAC3B,kBAAC,EAAD,a,GAdmCd,aCH1Be,E,uKAEjB,OACE,yBAAK5B,MAAO,CACVoB,UAAW,YAEX,uFAEA,mDACI,4BACA,4BAAI,uBAAGL,KAAK,MAAR,iBACJ,4BAAI,uBAAGA,KAAK,MAAR,2DACA,wBAAIC,KAAK,KACL,4BAAI,uBAAGD,KAAK,OAAR,uBACJ,4BAAI,uBAAGA,KAAK,OAAR,iCAIZ,4BAAI,uBAAGA,KAAK,MAAR,8CACA,wBAAIC,KAAK,KACL,4BAAI,uBAAGD,KAAK,OAAR,aACJ,4BAAI,uBAAGA,KAAK,OAAR,iCACJ,4BAAI,uBAAGA,KAAK,OAAR,kBACJ,4BAAI,uBAAGA,KAAK,OAAR,eAIZ,4BAAI,uBAAGA,KAAK,MAAR,YACJ,4BAAI,uBAAGA,KAAK,MAAR,YACA,wBAAIC,KAAK,KACL,4BAAI,uBAAGD,KAAK,OAAR,iBACJ,4BAAI,uBAAGA,KAAK,OAAR,mCAGZ,4BAAI,uBAAGA,KAAK,MAAR,gBAIR,wBAAIE,GAAG,KAAP,oBACA,6BACE,yBAAKA,GAAG,OAAOb,UAAU,cACvB,yBACED,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,qCADF,sOAQF,6uCAmBA,oOAMA,0TAMA,meAUF,wBAAIgB,GAAG,KAAP,8DACA,wBAAIA,GAAG,MAAP,0BACA,6BAEE,yBAAKA,GAAG,OAAOb,UAAU,cACvB,yBACED,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,qCADF,4KAQF,0gBAQA,0fAUF,wBAAIgB,GAAG,MAAP,kCAGA,uqBAWA,0GAEE,4BACE,8TAMA,6KAIA,sWAOA,qSAOA,yNAKA,wHAOJ,wBAAIA,GAAG,KAAP,iDACA,6BACE,6BAEE,yBAAKA,GAAG,OAAOb,UAAU,kBAAzB,KAEE,yBACED,IAAK,sBACLH,MAAO,CACPC,MAAO,UALX,KAQE,yBACEE,IAAK,sBACLH,MAAO,CACPC,MAAO,UAXX,KAcE,yBACEE,IAAK,sBACLH,MAAO,CACPC,MAAO,UAjBX,KAoBE,yBACEE,IAAK,sBACLH,MAAO,CACPC,MAAO,UAvBX,KA0BE,yBACEE,IAAK,sBACLH,MAAO,CACPC,MAAO,UAET,6BACE,qCADF,kKAUJ,wBAAIgB,GAAG,MAAP,gBAEA,y3BAaA,yPAKA,8DAEE,4BACE,mLAIA,0OAKA,0IAIA,kMAUN,yBAAKjB,MAAO,CAAEY,QAAS,iBACrB,wBAAIK,GAAG,MAAP,oCACA,4mBAUA,yBAAKA,GAAG,OAAOb,UAAU,cACvB,yBACED,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,sCADF,0DAMF,8EAGA,sCACA,weASA,wBAAIgB,GAAG,MAAP,qBACA,4MAKA,sCACA,0bASA,wBAAIA,GAAG,MAAP,gBACA,84BAcA,yBAAKA,GAAG,QACN,yBACEd,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,sCADF,mFAQJ,6BACA,6BACA,6BACA,wBAAIgB,GAAG,KAAP,cACA,iTAKE,4BAEE,iMAIA,kNAKA,oWAMA,kQAKA,uUAQF,yBAAKA,GAAG,QACN,yBACEd,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,sCADF,sBAOJ,6BACA,6BACA,6BACA,wBAAIgB,GAAG,KAAP,eACA,wBAAIA,GAAG,MAAP,mBACA,6jBASA,6IAIA,0lBAUA,kYAOA,ghBAUE,yBAAKA,GAAG,QACN,yBACEd,IAAK,qBACLH,MAAO,CACPC,MAAO,UAET,6BACE,sCADF,uGAQJ,6BACA,wBAAIgB,GAAG,MAAP,yDACA,kxBAYA,wLAIA,gkBASA,8MAMA,wBAAIA,GAAG,KAAP,kBACA,moBAWA,6BACA,6BACA,6BACA,4CAjgBF,yWAugBE,6BAvgBF,0MA2gBE,6BA3gBF,mMA+gBE,6BA/gBF,iPAmhBE,6BAnhBF,8S,GAHuCJ,aCAxBgB,E,uKAEjB,OACE,6BACE,8D,GAJuChB,aCE1BiB,E,uKAEjB,OACI,kBAACL,EAAA,EAAIM,UAAL,CAAed,GAAG,oBAAoBM,iBAAiB,SACnD,kBAACS,EAAA,EAAD,KACI,kBAACC,EAAA,EAAD,CAAKC,GAAI,GACT,kBAACC,EAAA,EAAD,CAAKC,QAAQ,OAAOhC,UAAU,eAC1B,kBAAC+B,EAAA,EAAIE,KAAL,KACI,kBAAC,IAAD,CAAWC,GAAG,aAAalC,UAAU,uBAC1BmC,YAAa,CACRC,OAAQ,kBACRC,aAAc,QAElBC,OAAK,GALjB,gBAUJ,kBAACP,EAAA,EAAIE,KAAL,KACI,kBAAC,IAAD,CAAWC,GAAG,qBAAqBlC,UAAU,uBAClCmC,YAAa,CACRC,OAAQ,kBACRC,aAAc,QAElBC,OAAK,GALjB,aASJ,kBAACP,EAAA,EAAIE,KAAL,KACI,kBAAC,IAAD,CAAWC,GAAG,uBAAuBlC,UAAU,uBACpCmC,YAAa,CACRC,OAAQ,kBACRC,aAAc,QAElBC,OAAK,GALjB,iBAWR,kBAACT,EAAA,EAAD,CAAKC,GAAI,GACT,kBAACT,EAAA,EAAIkB,QAAL,KACI,kBAAC,IAAD,CAAOC,KAAK,aAAaC,UAAW1B,EAAmBuB,OAAK,IAC5D,kBAAC,IAAD,CAAOE,KAAK,qBAAqBC,UAAWjB,EAAiBc,OAAK,IAClE,kBAAC,IAAD,CAAOE,KAAK,uBAAuBC,UAAWhB,EAAmBa,OAAK,Y,GA5CpD7B,aCFjBiC,E,uKAEjB,OACE,yBAAK9C,MAAO,CAAC+C,cAAc,WACvB,2BAAO9B,GAAG,IAAIhB,MAAM,OAAOD,MAAO,CAACoB,UAAU,OAAQ2B,cAAc,SAAUC,OAAO,SAClF,+BACI,4BACI,4BAAI,2CAER,4BACI,wBAAI/C,MAAM,OACN,qEAA6C,6BAC7C,yBAAKG,UAAU,YAAf,4BAGI,uBAAGW,KAAK,yDAAyDkC,OAAO,UAAxE,kCAHJ,OAQI,uBAAGlC,KAAK,uIAAuIkC,OAAO,UAAtJ,iDARJ,OAYI,uBAAGlC,KAAK,6CAA6CkC,OAAO,UAA5D,kCAZJ,OAgBI,uBAAGlC,KAAK,mHAAmHkC,OAAO,UAAlI,uCAhBJ,OAoBI,uBAAGlC,KAAK,sCAAsCkC,OAAO,UAArD,qBApBJ,OAwBI,uBAAGlC,KAAK,4BAA4BkC,OAAO,UAA3C,gCAxBJ,OA4BI,uBAAGlC,KAAK,sHAAsHkC,OAAO,UAArI,+BA5BJ,OAgCI,uBAAGlC,KAAK,oJAAoJkC,OAAO,UAAnK,iCAhCJ,OAoCI,uBAAGlC,KAAK,+DAA+DkC,OAAO,UAA9E,qCApCJ,OAwCI,uBAAGlC,KAAK,sGAAsGkC,OAAO,UAArH,wCAxCJ,OA4CI,uBAAGlC,KAAK,oHAAoHkC,OAAO,UAAnI,4BAGA,6BA/CJ,4BAoDJ,wBAAIhD,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,iBAA2D,6BAA3D,cAGJ,4BACI,wBAAIpB,MAAO,CAACkD,WAAW,SAAS,4CAEpC,4BACI,wBAAIjD,MAAM,OACN,2BACI,uBAAGc,KAAK,+BAA+BkC,OAAO,UAA9C,mCAKJ,yBAAK7C,UAAU,YAAf,6DAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,iBAA2D,6BAA3D,qCAGJ,4BACI,4BACI,2EACI,uBAAGL,KAAK,2CAA2CkC,OAAO,UAA1D,sDAIJ,+EAMR,4BACI,wBAAIhD,MAAM,OACN,kDAIA,yBAAKG,UAAU,YAAf,gCAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,kBAEI,6BAFJ,mBAMJ,4BACI,4BACI,qJACA,0GAMR,4BACI,wBAAInB,MAAM,OACN,2BACI,uBAAGc,KAAK,sBAAsBkC,OAAO,UAArC,+BAKJ,yBAAK7C,UAAU,YAAf,yEAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,gBAEI,6BAFJ,8BAMJ,4BACI,4BACI,gHAEA,uEAEI,uBAAGL,KAAK,6DAA6DkC,OAAO,UAA5E,2DASZ,4BACI,wBAAIhD,MAAM,OACN,kCAIA,yBAAKG,UAAU,YAAf,8BAEI,uBAAGW,KAAK,4BAA4BkC,OAAO,UAA3C,sDAMR,wBAAIhD,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,gBAEI,6BAFJ,sCAMJ,4BACI,4BACI,wFAEA,6LAMR,4BACI,wBAAInB,MAAM,OACN,2BACI,uBAAGc,KAAK,wBAAwBkC,OAAO,UAAvC,yBAKJ,yBAAK7C,UAAU,YAAf,gCAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,kBAEI,6BAFJ,gBAMJ,4BACI,4BACI,mLAEA,6QAMR,4BACI,4BAAI,4CAER,4BACI,wBAAInB,MAAM,OACN,2BACI,uBAAGc,KAAK,wBAAwBkC,OAAO,UAAvC,0CAKJ,yBAAK7C,UAAU,YAAf,YAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,iBAA2D,6BAA3D,SAGJ,4BACI,wBAAInB,MAAM,OACN,2BACI,uBAAGc,KAAK,yCAAyCkC,OAAO,UAAxD,+DAMJ,yBAAK7C,UAAU,YAAf,2BAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,oBAEI,6BAFJ,cAQJ,4BACI,wBAAInB,MAAM,OACN,6DAGI,uBAAGc,KAAK,2BAA2BkC,OAAO,UAA1C,6BAKJ,yBAAK7C,UAAU,YAAf,wBAKJ,wBAAIH,MAAM,MAAMD,MAAO,CAACoB,UAAU,WAAlC,oBAEI,6BAFJ,gBAYV,2BAAOH,GAAG,IAAIhB,MAAM,OAAOD,MAAO,CAACoB,UAAU,OAAQ2B,cAAc,SAAUC,OAAO,SAC9E,4BACI,4BAAI,wCAGR,4BACI,wBAAI/C,MAAM,OAAV,IAAiB,0CAAjB,KACA,wBAAIA,MAAM,OAAV,oFAIJ,4BACI,wBAAIA,MAAM,OAAV,IAAiB,uCAAjB,KACA,wBAAIA,MAAM,OAAV,+EAIJ,4BACI,wBAAIA,MAAM,OAAV,IAAiB,wCAAjB,KACA,wBAAIA,MAAM,OAAV,oDAIJ,4BACI,wBAAIA,MAAM,OAAV,IAAiB,uCAAjB,KACA,wBAAIA,MAAM,OAAV,0FAIJ,4BACI,wBAAIA,MAAM,OAAV,IAAiB,2CAAjB,KACA,wBAAIA,MAAM,OAAV,mF,GA7TgBY,aCAfsC,E,uKAEjB,OACI,yBAAKnD,MACD,CACIoD,WAAY,yBACZC,eAAgB,UAChBC,iBAAkB,YAClBrD,MAAM,OACNO,OAAO,SACP+C,MAAO,UAKX,yBAAKnD,UAAU,gBACX,sZAII,6BAJJ,4F,GAhBsBS,a,MC4CvB2C,MAvCf,WAEE,OACE,kBAAC,IAAD,CAAQC,SAAUC,IAChB,yBAAKtD,UAAU,sBACb,yBAAKA,UAAU,UACf,yBAAKA,UAAU,QACb,kBAAC,IAAD,CAASkC,GAAG,IAAII,OAAK,EAAC1C,MACpB,CACEuD,MAAO,QACPI,eAAgB,SAGlB,8CAGJ,yBAAKvD,UAAU,QAAQJ,MAAO,CAAC4D,SAAS,OAClC,kBAAC,IAAD,CAAStB,GAAG,UAAUlC,UAAU,UAAUmC,YAAa,CAACsB,aAAc,mBAAoBnB,OAAK,GAA/F,UAGA,kBAAC,IAAD,CAASJ,GAAG,YAAYlC,UAAU,UAAUmC,YAAa,CAACsB,aAAc,oBAAxE,YAGA,kBAAC,IAAD,CAASvB,GAAG,QAAQlC,UAAU,UAAUmC,YAAa,CAACsB,aAAc,oBAApE,UAOJ,kBAAC,IAAD,CAAOjB,KAAK,IAAIC,UAAWM,EAAUT,OAAK,IAC1C,kBAAC,IAAD,CAAOE,KAAK,UAAYC,UAAWC,EAAQJ,OAAK,IAChD,kBAAC,IAAD,CAAOE,KAAK,YAAYC,UAAWf,OC7BzBgC,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCZNC,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,EAAD,OAEFC,SAASC,eAAe,SDyHpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,c","file":"static/js/main.f09d0362.chunk.js","sourcesContent":["\r\nimport React, { Component } from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\n\r\nexport default class MazeSolverOverview extends Component {\r\n  render() {\r\n    return (\r\n        <div>\r\n\r\n        <img style={\r\n          {\r\n            width:\"50%\",\r\n            float: 'right'\r\n          }\r\n        }  src={'/maze_solver/wall3d.png'}/>\r\n\r\n        <p>\r\n          The Maze Challenge was the cultimation of the \"Robotics: Science and Systems\" class.\r\n          The challenge is to program Wall3d (the robot in the image) to get out of an unexplored maze as fast as possible while minimizing collisions.\r\n        </p>\r\n        <p>\r\n          To accomplish this task, Wall3D uses SLAM, path planning, trajectory following, safety controllers, and frontier exploration.\r\n        </p>\r\n        <p> \r\n          Our robot got the best score in the Maze Challenge. Most other approaches relied on some variation of wall following.</p>\r\n        <p>\r\n          Here is a video that goes through the high level explanation of how our robot works:\r\n        </p>\r\n\r\n        <div className=\"video-wrapper\">\r\n          <iframe className=\"video-frame\" \r\n                  src=\"https://www.youtube.com/embed/PrP86YrPoR0\" \r\n                  frameborder=\"0\" \r\n                  allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \r\n                  allowfullscreen>\r\n          </iframe>\r\n        </div>\r\n\r\n        <p>\r\n          The sildes for more details on how we use Google Cartografer for SLAM and how we process the map \r\n          in order to be able to perform A*, and subsequently trajectory following, efficiently:\r\n        </p>\r\n        <div className=\"video-wrapper\">\r\n          <iframe className=\"video-frame\" \r\n                  src=\"https://docs.google.com/presentation/d/e/2PACX-1vTqXaYhTXgwJ_DPO3bpJjTPzB6wHcrB4mNkEAurd-2MHXkMApfmxKwZkfwkg2wChygbciwCbMuz_3Cd/embed?start=false&loop=false&delayms=3000\" \r\n                  frameborder=\"0\" \r\n                  width=\"960\" \r\n                  height=\"569\" \r\n                  allowfullscreen=\"true\" \r\n                  mozallowfullscreen=\"true\" \r\n                  webkitallowfullscreen=\"true\">\r\n          </iframe>\r\n        </div>\r\n\r\n        <p>\r\n            One very important thing we learned in this class is that a robot is a complex system, and it can fail in many unexpected ways, as seen in this video:\r\n        </p>\r\n        <div className=\"video-wrapper\">\r\n          <iframe className=\"video-frame\" \r\n                  src=\"https://www.youtube.com/embed/bDDuAJvdg-M\" \r\n                  frameborder=\"0\" \r\n                  allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \r\n                  allowfullscreen>\r\n          </iframe>\r\n        </div>\r\n\r\n        <p>\r\n            Nevertheless, after dozens of hours of relentless tweaking, we got pretty good results. This was our final run:\r\n        </p>\r\n\r\n        <div style={{width:\"100%\", position:'relative'}}>\r\n          <div>\r\n            <div style={{width:'50%', display:'inline-block'}}>Rviz visualization:</div> \r\n            <div style={{width:'50%', display:'inline-block'}}>Reality:</div> \r\n\r\n          </div>\r\n          <div>\r\n            <div style={{float:'left', width:'50%'}}>\r\n              <iframe width=\"100%\" src=\"https://www.youtube.com/embed/Otesxus6TXY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\r\n            </div>\r\n            <div style={{float:'right', width:'50%'}}>\r\n              <iframe width=\"100%\" src=\"https://www.youtube.com/embed/alsAfsBNwQI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\r\n            </div>\r\n          </div>\r\n        </div>\r\n\r\n        <h4>Conclusion</h4>\r\n        <p>\r\n          RSS was one of the best classes I took at MIT. It was very demanding (aprox. 20 hours/week on average) but the amount of knowledge per hour learned in this class was unparalleled.\r\n          Most of the time was spent together with the team working on lab assignments. I was personally responsible for slam (using cartographer) and path planning (A*). Other members of the team were responsible for safety controllers, path following (pure pursuit) and a plan B - a simpler implementation based on wall-following. \r\n        </p> \r\n\r\n        <img style={{width:'100%'}} src={\"/maze_solver/team.jpg\"}/>\r\n        </div>\r\n    )\r\n  }\r\n}\r\n\r\n","\r\nimport React, { Component } from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\nimport './css/mazeSolverProject.css'\r\n\r\nexport default class MazeSolverReport extends Component {\r\n  render() {\r\n    return (\r\n      <div>\r\n\r\n        <h5>\r\n            Table of Contents:           \r\n        </h5>\r\n            <ol>\r\n            <li><a href=\"#1\">Overview and Motivations</a></li>\r\n            <li><a href=\"#2\">Proposed Approach</a>\r\n                <ol type=\"a\">\r\n                    <li><a href=\"#2a\">Frontier exploration</a></li>\r\n                    <li><a href=\"#2b\">Path planning</a></li> \r\n                    <li><a href=\"#2c\">Trajectory tracking</a></li> \r\n                </ol>\r\n\r\n            </li>\r\n            <li><a href=\"#3\">Experimental Evaluation</a></li>\r\n            <li><a href=\"#4\">Lessons Learned</a></li>\r\n            <li><a href=\"#5\">Potential Improvements</a></li>\r\n\r\n            </ol>\r\n        \r\n        <div>\r\n            <h4 id=\"1\"> 1. OVERVIEW AND MOTIVATIONS </h4>\r\n\r\n            <div>\r\n            <img className=\"halfscreen\" src={'/maze_solver/maze_example.png'}/>\r\n            <p>\r\n                The labyrinth final challenge involves the car being placed in an unknown maze of a given radius and having to exit the maze in a time constraint. Figure 1 illustrates an example maze that could be used. This challenge uses simultaneous localization and mapping from the localization lab so the car can generate a map of the maze as it drives and keep track of where it is in the map. This challenge also utilizes a search-based path planning algorithm from the path planning lab to create a path that represents a solution to the maze and uses the pure pursuit algorithm from the same path planning lab to follow the solution path to the exit. It is given that the maze is multicursal and not simply connected, so the car is not guaranteed to exit the maze if it uses the basic maze solving method of following one wall. The main difficulties our team is anticipating when approaching this challenge includes how to efficiently explore the map and how to ensure the car can maneuver in tight spaces (i.e. 3-point turns when leaving dead ends).\r\n            </p>\r\n\r\n            </div>\r\n\r\n            <h4 id=\"2\">2. PROPOSED APPROACH </h4>\r\n            <p>\r\n                To solve the maze, we use frontier exploration to generate a map of the maze and once the car detects an area outside the radius of the maze, a path is generated for the car to follow to complete the maze. To search frontiers, our priority queue is based on proximity to the car’s current location because search algorithms such as breadth-first search and depth-first search require the car to make giant leaps from one frontier to another which is not instantaneous in real life. To follow the paths generated via Dijkstra’s algorithm from the car’s location to the nearest frontier, the car uses pure pursuit both forwards and backwards, depending on which direction is more optimal, considering the maze has fairly tight hallways.\r\n            </p>\r\n\r\n            <h5 id=\"2a\">2.a FRONTIER EXPLORATION</h5>\r\n                    <b>Processing the map</b>\r\n                    <div>\r\n                        <div>\r\n                            <div className=\"halfscreen\">\r\n                                <img style={{width:'100%'}} src={'/maze_solver/thin_walls.png'}/>\r\n                                <img style={{width:'100%'}} src={'/maze_solver/thick_walls.png'}/>\r\n                            </div>\r\n                        </div>\r\n                        <p>\r\n                            Our team uses Google Cartographer for simultaneous localization and mapping. Cartographer outputs a grid of 20 cells/meter, each cell being assigned a value of -1 if it is “unexplored” or a value from 0 to 100 representing the probability of that cell being occupied. To make this map more useful and easy to work with, we downsampled it to about 10 cm/cell. We then converted it to a true occupancy grid that only contains three values: -1, 0, and 100 for “unexplored”, free space, and occupied, respectively. Additionally, we “thickened” the walls and assigned a cost to each cell based on its distance from a wall in order to avoid wall collisions.\r\n                        </p>\r\n\r\n                    </div>\r\n            <ul>\r\n                <li>\r\n                    <b>Downsampling and converting the map into a true occupancy grid.</b>\r\n                    <p>\r\n                        First, we define a downsampling factor. Let’s call it d. Each cell in the downsampled map is d x d cells in the original map. We consider a cell in the original map to represent a wall if its value is greater than 50. If at least 10% of the “original” cells in the downsampled cell are occupied (value > 50), then we mark the downsampled cell as occupied as well.\r\n                    </p>\r\n                </li>\r\n                <li>\r\n                    <b>Thickening the walls</b>\r\n                    <p>\r\n                        In order to “thicken” the walls on the map, we defined a procedure that creates a copy of the map, iterates through every cell, and if the cell has at least two occupied neighboring cells on the original map, the cell is marked as occupied as well. At the end, the copy is saved and the old map is removed. We repeat this process three times in order to ensure our map is sufficiently dilated.\r\n                    </p>\r\n                </li>\r\n                <li>\r\n                    <b>Assigning costs to the cells on the map </b>\r\n                    <p>\r\n                        If one just runs BFS on an occupancy grid, the shortest path will have sections that are very close to walls. In order to avoid this, we assign a cost to each cell based on its distance to the closest wall. The closer to a wall a cell is, the higher the associated cost. The cost of a cell we use is 2^20/2^(# cells from wall). Any cell that is more than 20 cells away from the nearest wall is assigned a cost of 0.\r\n                    </p>\r\n                </li>\r\n            </ul>\r\n\r\n            <h5 id=\"2b\">2.b PATH PLANNING</h5>\r\n            <div>\r\n                <div>\r\n                    <div className=\"halfscreen\">\r\n                        <img style={{width:'100%'}} src={'/maze_solver/djikstra.gif'}/>\r\n                    </div>\r\n                </div>\r\n                <p> \r\n                    For path planning, we use Dijkstra’s algorithm. The algorithm returns the path of minimal cost to the closest “unexplored” cell (with value -1) from the current position of the car. The cost of a path is the sum of the costs of the cells along it. \r\n                </p>\r\n                <p>\r\n                    Dijkstra’s algorithm performs frontier exploration until it finds a destination. It always expands the frontier to the point of minimal distance, or cost, from the starting point.\r\n                </p>\r\n                <p>\r\n                    Due to our lidar not being able to see backwards, it is highly preferable that the paths generated be directed forward. Therefore, we draw an imaginary wall behind the car on the map and then run our path finding algorithm. If the algorithm can’t find any path, we remove the wall and try generating a path again.\r\n                </p>\r\n            </div>\r\n\r\n            <h5 id=\"2c\">2.c TRAJECTORY TRACKING</h5>\r\n            <p>\r\n                For trajectory tracking, we use the pure pursuit algorithm from the path planning lab. To reiterate from last lab, the pure pursuit algorithm begins by finding the segment in the trajectory that is closest to the car. Then starting from the closest segment, the algorithm iterates through the rest of the segments in the trajectory until a segment that contains a point exactly one “lookahead distance” away from the car is found. The steering angle commanded to the car to follow the trajectory then becomes a function of the lookahead distance, the angle from the line segment made from the goal point and the car position to the car, and the car’s length. The geometry and equations used for the pure pursuit algorithm are illustrated in the image below.\r\n            </p>\r\n            <img style={{width:'100%'}} src={'/maze_solver/pure_pursuit.png'}/>\r\n            <p>\r\n                Since the maze has tight hallways, the car cannot always make a full turn to start following a trajectory forwards without hitting a wall. This is especially the case when the trajectory and the car are facing in opposite directions as shown on the right of Figure ___. One possible solution to this issue is to implement a three-point turn every time the car needs to turn around. However, our car uses a Velodyne lidar to collect its scan data, and this lidar cannot detect objects within 0.5 meters. This means that if the car were to begin a three-point turn, it would not be able to detect how close it is to a wall in order to change the direction of driving.\r\n            </p>\r\n            <div>\r\n                <div>\r\n                    <div className=\"halfscreen\">\r\n                        <img style={{width:'100%'}} src={'/maze_solver/forwards_backwards.png'}/>\r\n                    </div>\r\n                </div>\r\n                <p>\r\n                    Therefore, we decided to incorporate pure pursuit in reverse whenever the trajectory was behind the car. All of the calculations for pure pursuit in reverse are the same as before, except the sign of the final steering angle and speed the car is commanded to drive is reversed. The metric we use in order to determine if the trajectory is in front or behind the car was the difference in angle between the first segment of the trajectory and the car’s pose, both in the world frame. As shown in figure __, if the difference in angle is more than π/2, then that means the car is facing in the opposite direction of the trajectory, and pure pursuit in reverse should be used over the regular pure pursuit algorithm.\r\n                </p>\r\n\r\n            </div>\r\n\r\n            <h4 id=\"3\">3. EXPERIMENTAL EVALUATION</h4>\r\n            <p>\r\n                We chose to focus our testing of the Maze Solver on real life scenarios. Due to the increased accuracy of Google Cartographer in simulation and the near-sightedness of the 3D Lidar, we decided that testing on the car would give us a better understanding of its performance for tuning and debugging purposes. We chose to evaluate our car in one maze. The maze included islands, dead ends, and tight passageways so as to mimic all possible maze scenarios. In order to collect as much data as possible, we varied the starting position of the car in the maze and measured its performance in all cases. \r\n            </p>\r\n            <p>\r\n                We placed our car in 3 positions in the maze. For each position, we measured the distance of the shortest path from the starting position to the goal. We used this distance as a baseline measure for our car. For each of the 3 positions, we ran the maze solver 3 times. We timed the maze solver and we measured (approximately) the total distance traveled by the car. We compared these distances to the baseline of that position. \r\n            </p>\r\n            <p>\r\n                Figure 7 indicates the approximate trajectories of the robot compared to the what the ideal trajectory would be given knowledge of the maze in advance. The trajectory of Path 1 is almost identical to that of the ideal trajectory because the robot heads towards open space and never arrives at a junction where it must decide where to go. The trajectory of Path 2 requires some extra distance covered due to the orientation of the robot. The robot is compelled to move forward until occupied space encourages the robot to turn around and head towards the ideal trajectory. Lastly, in Path 3, the robot is faced with a junction and choses to head towards a dead end before heading in the right direction. The robot also takes a longer route to get to the goal point after heading in the right direction. This explains why the standardized distance traveled is significantly greater for Path 3. Overall, standard deviation (as indicated by the error bars) for time traveled (Fig [where the time graph is]) and for distance traveled (Fig [where distance graph is]) is relatively low. This is because the frontier exploration algorithm is deterministic. Each time the robot begins in the same location and orientation, it will explore the same frontiers in the same order. \r\n            </p>\r\n\r\n            <div style={{width:'100%', marginBottom:'30px'}}>\r\n                <img src={'/maze_solver/experiments.jpg'} style={{width:'100%'}}/>\r\n            </div>\r\n\r\n            <h4 id=\"4\">4. LESSONS LEARNED </h4>\r\n            <p>\r\n                Over the course of this challenge, we learned the importance of prioritizing tasks and having a back-up algorithm. \r\n            </p>\r\n            <p>\r\n                At times in the lab, we focused on making the path finding and pure pursuit algorithms more robust while the basic movements of the maze solver did not work as anticipated. For example, we tried to implement dubens curves prior to having a working maze solver. This attempt to improve an already broken algorithm delayed our finding of a minimum viable product, costing the team invaluable time.\r\n            </p>\r\n            <p>\r\n                At the same time, we learned that having a simple backup algorithm is essential should the more robust algorithm not be completed. For this task, we realized that a stochastic wall follower algorithm could replace our pure pursuit controller to guide the car through the maze. While a wall follower would most likely take more time to solve a maze, it would at least be able to explore the maze.\r\n            </p> \r\n\r\n            <h4 id=\"5\">5. POTENTIAL IMPROVEMENTS</h4>\r\n            <p>\r\n                Our current implementation of a maze solver works well, but it has some issues and there are a lot of improvements we could implement in order to make it more robust. \r\n            </p>\r\n            <p>\r\n                The dynamics of the car itself are really important for path planning, but our current implementation does not take these into account. Our current path-planning algorithm for the maze works by using nodes and making paths between these nodes using Dijkstra's algorithm. The nodes are just cells from our down-sampled map, and the cost associated with them is determined by how close the cell is to a wall. By finding the least-cost path through this maze, we try to avoid walls. The issue with this implementation is that it is purely based upon cell cost. The turning radius of the car is never taken into account. As a result, we sometimes publish paths that the car cannot physically follow. We currently rely on our safety controller to get us out of these situations where the car could collide with an object. Using another representation of trajectories such as *dubin* curves would enable us to create smoother paths that would help both avoiding collisions as well as driving with pure pursuit. Having these smoother paths that take into account car dynamics would also allow us to increase the speed at which the car travels. Currently the car explores the car explores the maze slowly because of these uneven paths and to allow the safety controller to kick in soon enough to avoid obstacles. \r\n            </p>\r\n            <p>\r\n                Another issue we ran into with our current implementation is that because our current generated paths are only dependent upon low-cost and nearest frontier, we run into the issue where we cannot find a goal point for pure pursuit because our lookahead distance is static. Sometimes the start point of the trajectory is slightly too far from the car and sometimes the end of the trajectory is too close (because the maze environment is concave, the absolute distance between the start and end of a path can be less than one lookahead distance). Our current solution for this problem is generating a new path when we cannot find a goal point for the current one. This is not the most efficient solution, as this means we are no longer exploring the closest frontier and it is computationally expensive to generate new paths, which takes time (as can be seen when the car pauses in the maze to generate new paths). A better solution would be to make a dynamic look-ahead distance and change it accordingly based upon what path is generated and the position of the car with respect to that path.\r\n            </p>\r\n            <p>\r\n                Our current lidar is only capable of seeing objects with a minimum distance of 0.5 meters. If an object is closer than 0.5 meters, then the lidar views this as an object that is an infinite distance away. We currently run into issues where our car will re-map a previously discovered wall as not existing because it gets within 0.5 meters of it and thinks that nothing is there. As a result, sometimes paths are generated that go through walls. This happens very rarely, but when it does we rely on wall dilation as well as our safety controller. One proposed solution to this problem is that when we receive a scan of distance infinity from our lidar, we change it to a random number between 0.0 and 0.5 using a uniform distribution. This would solve the issue of deleting/re-mapping pre-existing walls as well as help us avoid generating paths too close to walls. The problem with this is that if a path in the maze is 1 meter or less wide, our car would never be able to traverse it, and would always map that something exists there. Another solution is to find these specific laser scans in the data, and remove them before we send the lidar data to Google Cartographer. This means that no scans that are of a value of infinity would be used, avoiding the remapping of walls. We will test both solutions in future to determine which one is more optimal, but it may vary based upon environment.\r\n            </p>\r\n            <p>\r\n                As previously discussed, we want our car to avoid bumping into walls/obstacles at all costs. Currently we dilate our walls using a nearest-neighbors method. This does not always work, as sometimes there are paths being generated that go too close to walls. In the future, we want to dilate our map more to avoid paths that are too close to walls, but we also have to figure out a way to not dilate so much that narrow passages are blocked off. Dilating the map works well for mazes that are more open, but for mazes that are more narrow, it could block off some of the maze.\r\n            </p>\r\n            <p>\r\n                Lastly, we want to implement a “drive to the end” functionality for the car. Our current path planning only looks for the nearest frontier, and creates a path there. However, if we are trying to escape a maze of a given radius, if we discover a point that is outside this given radius, we want to drive there immediately. We did not implement this because it was too computationally expensive to check if a point was discovered outside the radius every time the car wanted to plan a new path.\r\n            </p>\r\n            <ul>\r\n                <li>Creating paths that are more geared towards car maneuverability x</li>\r\n                <li>Dynamic lookahead distance for pure pursuit</li>\r\n                <li>Not using a velodyne for this maze - Improving upon velodyne remapping because a space is within 0.5m</li>\r\n                <li>If it discovers an open space far enough away, drive there immediately</li>\r\n                <li>Computationally expensive</li>\r\n                <li>Dilating walls more to avoid wall collisions, improving safety controller</li>\r\n                <li>Faster maze movement (but cartographer wacks out)</li>\r\n            </ul>\r\n        </div>\r\n      </div>\r\n    )\r\n  }\r\n}\r\n\r\n\r\n\r\n","import React, { Component } from 'react'\r\nimport {Tab, Tabs} from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\nimport MazeSolverOverview from './mazeSolverOverview'\r\nimport MazeSolverReport from './mazeSolverReport'\r\nimport './css/mazeSolverProject.css'\r\n\r\nexport default class MazeSolverProject extends Component {\r\n  render() {\r\n    return (\r\n      <div style={{\r\n        textAlign: \"justify\"\r\n      }}>\r\n        <h2> Wall3D: Maze Solving Challenge </h2>\r\n        <p align=\"middle\"><i>Team 9: Talia Pelts, Kevin Carlson, Jonathan Samayoa, Susan Ni, Vlad Seremet</i></p>\r\n\r\n        <Tabs defaultActiveKey=\"overview\" style={{fontWeight:\"bold\"}} id=\"uncontrolled-tab-example\">\r\n          <Tab eventKey=\"overview\" title=\"General Overview\">\r\n            <MazeSolverOverview/>\r\n          </Tab>\r\n          <Tab eventKey=\"report\" title=\"Technical Report\">\r\n            <MazeSolverReport />\r\n          </Tab>\r\n        </Tabs>\r\n\r\n\r\n      </div>\r\n    )\r\n  }\r\n}\r\n\r\n\r\n\r\n","import React, {Component} from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\n\r\nexport default class HarvestRProject extends Component {\r\n  render() {\r\n    return (\r\n      <div style={{\r\n        textAlign: \"justify\"\r\n      }}>\r\n        <h2>HarvesR: using reinforcement learning for fruit picking</h2>\r\n\r\n        <h5> Table of Contents:</h5>\r\n            <ol>\r\n            <li><a href=\"#1\">Introduction</a></li>\r\n            <li><a href=\"#2\">Robot Architectore and the Unity Simulation Environment</a>\r\n                <ol type=\"a\">\r\n                    <li><a href=\"#2a\">Robot Architecture</a></li>\r\n                    <li><a href=\"#2b\">The Simulation Environment</a></li> \r\n                </ol>\r\n            </li>\r\n\r\n            <li><a href=\"#3\">Internal Model Representation and Planning</a>\r\n                <ol type=\"a\">\r\n                    <li><a href=\"#3a\">Overview</a></li>\r\n                    <li><a href=\"#3b\">Encoder-Decoder Architecture</a></li> \r\n                    <li><a href=\"#3c\">Forward Model</a></li> \r\n                    <li><a href=\"#3d\">Planning</a></li> \r\n                </ol>\r\n            </li>\r\n\r\n            <li><a href=\"#4\">Control</a></li>\r\n            <li><a href=\"#5\">Training</a>\r\n                <ol type=\"a\">\r\n                    <li><a href=\"#5a\">Pre-Training</a></li>\r\n                    <li><a href=\"#5b\">Unity and the training setup</a></li> \r\n                </ol>\r\n            </li>\r\n            <li><a href=\"#6\">Conclusion</a></li>\r\n\r\n            </ol>\r\n\r\n        <h4 id=\"1\">1. Introduction </h4>\r\n        <div>\r\n          <div id=\"fig1\" className=\"halfscreen\">\r\n            <img\r\n              src={'/harvestr/fig1.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 1\r\n              </b>\r\n              &nbsp; Averages and range of reported quantitative performance indicators:\r\n              localization success, detachment success, harvest success, fruit damage, and\r\n              peduncle damage. N represents the number of distinct projects. (Bac et al.)\r\n            </div>\r\n          </div>\r\n          <p>\r\n            Because fruit harvesting is classified as a low skill job, pundits routinely put\r\n            it in the category of jobs on the verge of automation. However, in accordance to\r\n            Moravec’s paradox, in actuality, fruit picking is much harder to automate than\r\n            many jobs that are regarded as “high skill”, such as accounting, radiography,\r\n            stock trading etc. In Morave’s words: \"it is comparatively easy to make\r\n            computers exhibit adult level performance on intelligence tests or playing\r\n            checkers, and difficult or impossible to give them the skills of a one-year-old\r\n            when it comes to perception and mobility\". This is in part due to the high\r\n            variability in the environment, and high adaptability required to conditions\r\n            such as fruit shape/distance/color, lighting, occlusion etc. A review of the\r\n            state of the art in fruit harvesting concluded that the performance of\r\n            harvesting robots has been stagnant for the past 30 years [1]. The same review\r\n            found that the average harvest success was 66% (Fig. 1) and the average cycle\r\n            time was 33s. It is worth mentioning that this data is skewed by easily\r\n            harvestable crops such as kiwis, apples, roses etc. For crops such as peppers,\r\n            tomatoes and strawberries the performance is even worse.\r\n          </p>\r\n\r\n          <p>\r\n            So far, harvesting robots have used classical algorithms for planning and\r\n            execution. As such, it is very hard to anticipate and hard code all the\r\n            conditions a harvesting robot might find itself in.\r\n          </p>\r\n\r\n          <p>\r\n            This paper proposes a different approach – one based on reinforcement learning.\r\n            Classical planning is still used for moving from one fruit to another, but once\r\n            the gripper is in proximity of a fruit, a neural network trained in simulation\r\n            is used to generate a control sequence.\r\n          </p>\r\n          <p>\r\n            Section 1 describes the architecture of the robot and the simulation environment\r\n            used for simulating the plant dynamics, and the robot-plant interactions.\r\n            Section 2 details the internal model used for planning, how this model is\r\n            acquired in simulation, how this model transfers from simulation to the real\r\n            world, and how this model is acquired in the real world. Section 3 discusses the\r\n            control algorithm. Section 4 discusses the training methodology.\r\n          </p>\r\n        </div>\r\n\r\n        <h4 id=\"2\">2. Robot architecture and the Unity simulation environment</h4>\r\n        <h5 id=\"2a\">2.a Robot Architecture</h5>\r\n        <div>\r\n\r\n          <div id=\"fig2\" className=\"halfscreen\">\r\n            <img\r\n              src={'/harvestr/fig2.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 2\r\n              </b>\r\n              &nbsp; 4 Degree of freedom (DOF) fruit-picking robot. Similar systems are used\r\n              in many harvesting robots, as it strikes a good balance between simplicity and\r\n              versatility\r\n            </div>\r\n          </div>\r\n          <p>Although the HarvestR architecture makes few assumptions about the\r\n            architecture of the robot, for demonstration and testing a 4DOF robot is used\r\n            (Fig.2). The robot can move across a raw of crops, up and down to harvest plants\r\n            at different heights, it can rotate in order to navigate between stems, leaves\r\n            and other obstacles and adapt to the position of the target fruit, and it can\r\n            extend its arm. Similar designs are used in most harvesting robots [1], as they\r\n            have very simple kinematics.\r\n          </p>\r\n          <p>\r\n            The action space is a 5-element vector of numbers between [-1,1] that specify\r\n            the motion of each DOF (plus gripper) in the interval [-maxSpeed*dt,\r\n            maxSpeed*dt], where maxSpeed is a parameter defined for each DOF. The\r\n            observation space (the data received from the simulation after each timestep)\r\n            consists of 1 RGB image, 1 depth map, 1 semantic map (each pixel has a label),\r\n            256x256 pixels each, and the 32x32x32 semantic occupancy grid. The sensors are\r\n            illustrated in Fig.3.\r\n          </p>\r\n        </div>\r\n        <h5 id=\"2b\">\r\n          2.b The Simulation Environment\r\n        </h5>\r\n        <p>\r\n          One of the main hurdles in the real-world application of reinforcement learning\r\n          algorithms is sample inefficiency. It requires millions of training examples for\r\n          a reinforcement learning algorithm to start performing well, which is\r\n          unrealistic to acquire in a real world environment. Fortunately, the computing\r\n          power available today allows us to simulate complex environments with\r\n          complicated dynamics. Moreover, with the recent advancements in reinforcement\r\n          learning, many simulation environments provide APIsthat make it easier to define\r\n          trainable agents and extract useful observation data (i.e. MuJoCo, PyBullet,\r\n          Gazebo, MATLAB Simulink, Unity).\r\n        </p>\r\n        <p>\r\n          For this particular application, Unity was chosen for the following reasons:\r\n          <ul>\r\n            <li>\r\n              Provides many off the shelf packages which makes development easier. For\r\n              example, Obi is a package for simulating ropes and rods – very useful for\r\n              simulating plant stems. Unity-voxel is a package that transforms meshes into\r\n              voxelgrids – useful for creating 3D occupancy grids.\r\n            </li>\r\n            <li>\r\n              ML-Agents Python API. An interface for defining agents in the simulation\r\n              environment, sending actions and receiving observations and rewards.\r\n            </li>\r\n            <li>\r\n              Very easy to train multiple agents in parallel. By simply copying the training\r\n              environment in the same scene, many clones of the same agent can be trained\r\n              simultaneously. Moreover, one can launch many simulations across multiple\r\n              machines, and control them from a central client using remote requests (for\r\n              example using (RPyC).\r\n            </li>\r\n            <li>\r\n              Has a large variety of predefined simulated sensors (i.e. ray vision, RGBD\r\n              cameras – most important for this application, Accelerometers, Pressure Sensors\r\n              etc.). Moreover, one can define their own sensors, such as a sensor for\r\n              retrieving an occupancy grid.\r\n\r\n            </li>\r\n            <li>\r\n              Has a C# scripting API that allows to easily create many randomized environments\r\n              based on prefabs – important for making the algorithm adaptable and transferable\r\n              to the real world.\r\n            </li>\r\n            <li>\r\n              Allows for the creation of manual controllers – important for imitation\r\n              learning.\r\n            </li>\r\n\r\n          </ul>\r\n        </p>\r\n        <h4 id=\"3\">3. Internal model representation and planning</h4>\r\n        <div>\r\n          <div>\r\n\r\n            <div id=\"fig3\" className=\"onethirdscreen\">\r\n              a)\r\n              <img\r\n                src={'/harvestr/fig3a.png'}\r\n                style={{\r\n                width: '100%'\r\n              }}/>\r\n              b)\r\n              <img\r\n                src={'/harvestr/fig3b.png'}\r\n                style={{\r\n                width: '100%'\r\n              }}/>\r\n              c)\r\n              <img\r\n                src={'/harvestr/fig3c.png'}\r\n                style={{\r\n                width: '100%'\r\n              }}/>\r\n              d)\r\n              <img\r\n                src={'/harvestr/fig3d.png'}\r\n                style={{\r\n                width: '100%'\r\n              }}/>\r\n              e)\r\n              <img\r\n                src={'/harvestr/fig3e.png'}\r\n                style={{\r\n                width: '100%'\r\n              }}/>\r\n              <div>\r\n                <b>Fig. 3\r\n                </b>\r\n                &nbsp; a) the transparent cube is a grid of colliders that constructs the\r\n                occupancy grid. b) camera view c) Depth Image d) Semantic labeling e) Occupancy\r\n                Grid\r\n              </div>\r\n            </div>\r\n          </div>\r\n\r\n          <h5 id=\"3a\">3.a Overview\r\n          </h5>\r\n          <p>\r\n            The HarvestR architecture assumes one camera mounted above the gripper, so that\r\n            it has a view of the area captured by the occupancy grid. When the robot is\r\n            deployed in a physical environment, the camera data is augmented before it is\r\n            fed into the policy network and forward model for planning. Mapping images\r\n            directly to actions is possible, but very sample inefficient. Using only a\r\n            convolutional neural network to map images to actions, the network would have to\r\n            learn a 3D representation of the environment implicitly, since the actions are\r\n            in 3D space. Instead, a better solution is using semi-supervised representation\r\n            learning with a variational encoder to map images into a latent space that\r\n            captures the relevant spatial information (section 2.2). This latent vector is\r\n            then used as input to a forward model for prediction and planning (section 2.3).\r\n          </p>\r\n          <p>\r\n            This latent space can easily be decoded into a semantic occupancy grid – each\r\n            cell has an integer label (i.e. 0 – empty, 1 - leaf, 2 – stem, 3 – fruit that is\r\n            not ripe, 4 – fruit that is ripe).\r\n          </p>\r\n          <p>\r\n            This approach has many benefits:\r\n            <ul>\r\n              <li>\r\n                The inference of the model from camera data can be separated from planning,\r\n                which allows the control system to adapt across different environments.\r\n              </li>\r\n              <li>\r\n                On a structural and semantic level, many plants are very similar (most have\r\n                stems, leaves, peduncles and fruits). Using a semantically labelled occupancy\r\n                grid allows the robot to capture this structure.\r\n              </li>\r\n              <li>\r\n                It is easy to build a forward model, policy network and value function based on\r\n                the latent representation.\r\n              </li>\r\n              <li>\r\n                Easy to interpret for humans. Feeding the latent representation into the decoder\r\n                essentially allows us to visualize in 3D what the robot is “seeing”.\r\n              </li>\r\n\r\n            </ul>\r\n          </p>\r\n\r\n        </div>\r\n\r\n        <div style={{ display: \"inline-block\" }}>\r\n          <h5 id=\"3b\">3.b Econder-Decoder Architecture</h5>\r\n          <p>\r\n            First, a disentangled variational encoder [5] is implemented - very similar to\r\n            the one used in [2], except unlike an autoencoder, the input and output are\r\n            different. The purpose of the encoder is to create an embedding – a vector that\r\n            contains enough information that it can be fed into the decoder, and the\r\n            original occupancy grid could be reconstructed. This encoder takes as input a\r\n            256x256 depth map and a 256x256 semantic label map and outputs a 32x32x32\r\n            semantic occupancy grid (Fig. 4). All this data can be obtained from simulation\r\n            so the training data is virtually unlimited.\r\n          </p>\r\n          <div id=\"fig5\" className=\"halfscreen\">\r\n            <img\r\n              src={'/harvestr/fig5.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 5 </b>\r\n               &nbsp; \r\n               Convolutional network for semantic labeling [3].\r\n            </div>\r\n          </div>\r\n          <p>\r\n            The loss function used to train the encoder is:\r\n          </p>\r\n          <p>FORMULA</p>\r\n          <p>\r\n            where  represents the parameters of the decoder,  - parameters of the encoder,\r\n            x - input, y - output, z - latent vector. The second term in the loss function\r\n            is a penalty for the latent vector departing from an isometric Gaussian\r\n            distribution, which essentially forces the latent vector to retain only the\r\n            elements that are essential. This is important, since this vector is used as\r\n            input to the forward model, policy network and value function.\r\n          </p>\r\n\r\n          <h5 id=\"3c\">3.c Forward Model</h5>\r\n          <p>\r\n            The function of the forward model is essentially to predict the future based on\r\n            past experience. Formally, a forward model is a function M parametrized by  ,\r\n            such that\r\n          </p>\r\n          <p>FORMULA</p>\r\n          <p>\r\n            t s is a concatenation of the latent vector from the encoder (128 elements) with\r\n            the joint positions of the robot (5 elements). t a is a 5 element vector with\r\n            actions for each joint. The input to the forward model therefore is a 138\r\n            element vector. t 1 s  is a concatenation between the latent vector and the\r\n            joint positions at t 1. The forward model is implemented as a feed-forward\r\n            neural network.\r\n          </p>\r\n\r\n          <h5 id=\"3d\">3.d Planning</h5>\r\n          <p>\r\n            The joint positions are appended to the latent vector and the resulting vector\r\n            is fed into the policy network and 5 actions are sampled. These actions are\r\n            concatenated with t s and fed into the forward model to generate 5 potential t 1\r\n            s  . This process is iterated 5 times until 3125 potential t 5 s  states are\r\n            acquired. The value function is used to select the top 5 states. The rest are\r\n            discarded. This process is iterated 10 times until t 50 s  is reached. The\r\n            value function is used to pick the top state and the trajectory s a s a s a a s\r\n            t t t t t t t t , , , , , ,......., ,       1 1 2 2 49 50  is given to\r\n            the controller to follow. The robot follows the given trajectory until exp | z |\r\n            ected observed t t   z  or the trajectory has been completed. t z is the\r\n            latent vector at time t.\r\n\r\n          </p>\r\n          <div id=\"fig4\">\r\n            <img\r\n              src={'/harvestr/fig4.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 5 </b>\r\n               &nbsp; \r\n               Encoder-decoder architecture and planning based on latent representation\r\n            </div>\r\n          </div>\r\n        </div>\r\n\r\n        <br/>\r\n        <br/>\r\n        <br/>\r\n        <h4 id=\"4\">4. Control</h4>\r\n        <p>\r\n          At the highest level, the control loop is essentially a state machine (Fig. 6)\r\n          The states colored in blue use policies obtained through reinforcement learning\r\n          for control. These states have their own policy network, reward functions, and\r\n          value functions associated with them.\r\n          <ul>\r\n\r\n            <li>\r\n              “Find Fruit” uses semantic labeling on camera images to find the nearest ripe\r\n              fruit. Once a fruit is found, control is passed to “Get Close”.\r\n            </li>\r\n            <li>\r\n              “Get Close” simply moves the robot towards the target fruit, once the target\r\n              fruit is in the range of the occupancy grid, control is passed to “move and\r\n              cut”.\r\n            </li>\r\n            <li>\r\n              “Move and cut” uses a trained policy, value function and the forward model to\r\n              generate trajectories towards accomplishing its goal – getting the gripper\r\n              around the peduncle of the target fruit, cutting it and holding it. Once this is\r\n              accomplished or interrupted, control is passed to “check status”.\r\n            </li>\r\n            <li>\r\n              “Check status” has no control over the joints. It simply assesses the situation\r\n              and moves to the appropriate state. If all is good and the fruit is held by the\r\n              gripper, control is passed to “Move and drop”.\r\n            </li>\r\n            <li>\r\n              “Move and drop” uses a policy network, a value function and a forward model to\r\n              generate trajectories towards accomplishing its goal: getting the fruit in the\r\n              basket. If the fruit reaches the basket, the control loop starts over, if not,\r\n              control is passed to “check status”.\r\n            </li>\r\n          </ul>\r\n        </p>\r\n          <div id=\"fig6\">\r\n            <img\r\n              src={'/harvestr/fig6.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 6 </b>\r\n               &nbsp; \r\n               Control loop\r\n            </div>\r\n          </div>\r\n\r\n        <br/>\r\n        <br/>\r\n        <br/>\r\n        <h4 id=\"5\">5. Training</h4>\r\n        <h5 id=\"5a\">5.a Pretraining</h5>\r\n        <p>\r\n          Since training the policy and value function is dependent on the encoder, the\r\n          segmentation convolutional net and the forward model, these need to be trained\r\n          first. For that purpose, many environments with different numbers and placements\r\n          of plants, different lighting conditions and textures are defined. The robot is\r\n          programmed to move around somewhat randomly. All the observations and actions\r\n          are stored in a buffer and the parameters of the encoder, forward model and\r\n          segmentation convolutional net are updated offline based on this data.\r\n        </p>\r\n        <p>\r\n          Next, the policy and value function of “move and cut” and “move and drop” are\r\n          initialized.\r\n        </p>\r\n        <p>\r\n          For “move and cut”, simple environments (with no overlapping stems) are\r\n          generated, and a path planning program is written to guide the gripper towards\r\n          the peduncle and cut it, while the observations, actions and rewards are\r\n          recorded and used to update the policy network and value function. This is\r\n          essentially done to give the robot a “head start” and to help it “understand”\r\n          that, for example, the gripper being around the peduncle of the target fruit is\r\n          a highly desirable position, while a collision with a stem or a leaf is\r\n          undesirable.\r\n        </p>\r\n        <p>\r\n          Similarly, for “move and drop”, environments are generated programmatically, the\r\n          robot is initialized with a fruit in its gripper and a program is written to\r\n          guide the robot towards the basket and drop the fruit, while the observations\r\n          and actions are stored into a buffer and used to update the policy network,\r\n          value function and forward model.\r\n        </p>\r\n        <p>\r\n          After the initialization is done, it’s time to move to actual training.\r\n          Environments are generated procedurally with randomization across multiple\r\n          parameters such as the starting position of the robot, number, positions, sizes\r\n          of plants, lighting conditions, textures etc. Trajectories and camera data are\r\n          saved for every time step and later used for updating the encoder, forward model\r\n          and the segmentation convolutional neural net. After these updates have been\r\n          made, the data is discarded.\r\n\r\n        </p>\r\n          <div id=\"fig7\">\r\n            <img\r\n              src={'/harvestr/fig8.png'}\r\n              style={{\r\n              width: '100%'\r\n            }}/>\r\n            <div>\r\n              <b>Fig. 7 </b>\r\n              &nbsp; \r\n              Training multiple environments in parallel. Each platform is a\r\n              separate training environment.\r\n            </div>\r\n          </div>\r\n\r\n        <br/>\r\n        <h5 id=\"5b\"> 5.b A few words about Unity and the training setup: </h5>\r\n        <p>\r\n          For the Unity ML-agents python API, a gym-like wrapper, UnityEnvWrapper, was\r\n          implemented in order to make it easier to interact with agents. In Unity it is\r\n          possible to create as many training environments as you want just by\r\n          instantiating a prefab environment in the same scene multiple times (Fig. 7),\r\n          and when env.step() is called, it returns a list of the agents that are waiting\r\n          for an action, together with their observations and rewards. Unity ML-agents is\r\n          a package that facilitates the use of Unity as a Reinforcement Learning\r\n          framework. It provides an Agent interface, which allows the user to define the\r\n          reward function, the observations collected at each step, the action space, how\r\n          the robot responds to each action, and other useful features.\r\n        </p>\r\n        <p>\r\n          A big hurdle was finding a good way to simulate the dynamics of plants, in\r\n          particular the movement of a plant stem when it is pushed around by the robot.\r\n        </p>\r\n        <p>\r\n          Experiments were performed with various designs based on chains of rigid bodies\r\n          connected with hinge joints, spring joints, custom joints etc. but to no avail.\r\n          They all resulted in either very unrealistic, stiff motion, or jittering and\r\n          instability on interaction, especially as the number of objects increased.\r\n          Thankfully there is a package called Obi, which defines ropes and rods with\r\n          adjustable rigidity, springiness, bending, stretching, tearing etc. With Obi\r\n          rods it was possible to create a plant that behaves realistically in simulation.\r\n        </p>\r\n        <p>\r\n          Another package used in this implementation is ImageSynthesis. This package\r\n          provides a very simple API for retrieving camera data together with depth maps\r\n          and semantic labels.\r\n\r\n        </p>\r\n        <h4 id=\"6\"> 6. Conclusion</h4>\r\n        <p>\r\n          Despite the immense progress in Reinforcement Learning and Machine Learning in\r\n          general, a seemingly simple task such as fruit picking still remains elusive.\r\n          This paper discussed why that is the case and proposed HarvestR – a control\r\n          system for harvesting robots based on deep reinforcement learning. At the heart\r\n          of HarvestR is a disentangled encoder that learns an embedding that captures the\r\n          relevant semantic data and spatial structure of a scene in a form that is easy\r\n          to feed into a state of the art RL algorithm such as PPO. The power of PPO is\r\n          increased by the use of a forward model for look-ahead .\r\n        </p>\r\n\r\n        <br/>\r\n        <br/>\r\n        <br/>\r\n        <h4>Bibliography</h4>\r\n        [1] Bac, C. Wouter, Eldert J. van Henten, Jochen Hemming, and Yael Edan.\r\n        “Harvesting Robots for High-Value Crops: State-of-the-Art Review and Challenges\r\n        Ahead: Harvesting Robots for High-Value Crops: State-of-the-Art Review and\r\n        Challenges Ahead.” Journal of Field Robotics 31, no. 6 (November 2014): 888–911.\r\n        https://doi.org/10.1002/rob.21525.\r\n        <br/>\r\n        [2] Schönberger, Johannes L., Marc Pollefeys, Andreas Geiger, and Torsten\r\n        Sattler. “Semantic Visual Localization.” ArXiv:1712.05773 [Cs], April 16, 2018.\r\n        http://arxiv.org/abs/1712.05773.\r\n        <br/>\r\n        [3] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. “Fully Convolutional\r\n        Networks for Semantic Segmentation.” ArXiv:1411.4038 [Cs], March 8, 2015.\r\n        http://arxiv.org/abs/1411.4038.\r\n        <br/>\r\n        [4] sagieppel. “Semantic Segmentation with Fully Convolutional Neural Network\r\n        (FCN) Pytorch Implementation.” github, n.d.\r\n        https://github.com/sagieppel/Fully-convolutional-neural-network-FCNfor-semantic-segmentation-with-pytorch.\r\n        <br/>\r\n        [5] Higgins, Irina, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles\r\n        Blundell, Shakir Mohamed, and Alexander Lerchner. “Early Visual Concept Learning\r\n        with Unsupervised Deep Learning.” ArXiv:1606.05579 [Cs, q-Bio, Stat], September\r\n        20, 2016. http://arxiv.org/abs/1606.05579.\r\n      </div>\r\n    )\r\n  }\r\n}","import React, { Component } from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\n\r\nexport default class PlatformerProject extends Component {\r\n  render() {\r\n    return (\r\n      <div>\r\n        <h2>Platformer: a fun 2D game</h2>\r\n      </div>\r\n    )\r\n  }\r\n}\r\n\r\n\r\n\r\n","import React, { Component } from 'react'\r\nimport {Tab, Col, Row, Nav} from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\nimport MazeSolverProject from './mazeSolverProject';\r\nimport HarvestRProject from './harvestRProject';\r\nimport PlatformerProject from './platformerProject';\r\nexport default class Projects extends Component {\r\n  render() {\r\n    return (\r\n        <Tab.Container id=\"left-tabs-example\" defaultActiveKey=\"first\">\r\n            <Row>\r\n                <Col lg={3}>\r\n                <Nav variant=\"tabs\" className=\"flex-column\">\r\n                    <Nav.Item>\r\n                        <NavLink   to=\"/projects/\" className=\"navlink navlink-left\"\r\n                                   activeStyle={{\r\n                                        border: '1px solid black', \r\n                                        borderRadius: '10px'\r\n                                        }}\r\n                                    exact>\r\n                                    Maze Solver\r\n                                    \r\n                        </NavLink>\r\n                    </Nav.Item>\r\n                    <Nav.Item>\r\n                        <NavLink   to=\"/projects/harvestr\" className=\"navlink navlink-left\"\r\n                                   activeStyle={{\r\n                                        border: '1px solid black', \r\n                                        borderRadius: '10px'\r\n                                        }}\r\n                                    exact>\r\n                        HarvestR\r\n                        </NavLink>\r\n                    </Nav.Item>\r\n                    <Nav.Item>\r\n                        <NavLink   to=\"/projects/platformer\" className=\"navlink navlink-left\"\r\n                                   activeStyle={{\r\n                                        border: '1px solid black', \r\n                                        borderRadius: '10px'\r\n                                        }}\r\n                                    exact>\r\n                        Platformer\r\n                        </NavLink>\r\n                    </Nav.Item>\r\n                </Nav>\r\n                </Col>\r\n                <Col lg={9}>\r\n                <Tab.Content>\r\n                    <Route path=\"/projects/\" component={MazeSolverProject} exact/>\r\n                    <Route path=\"/projects/harvestr\" component={HarvestRProject} exact/>\r\n                    <Route path=\"/projects/platformer\" component={PlatformerProject} exact/>\r\n                </Tab.Content>\r\n                </Col>\r\n            </Row>\r\n        </Tab.Container>\r\n    )\r\n  }\r\n}\r\n\r\n\r\n\r\n","import React, { Component } from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\n\r\nexport default class Resume extends Component {\r\n  render() {\r\n    return (\r\n      <div style={{verticalAlign:'center'}}>\r\n          <table id=\"t\" width=\"100%\" style={{textAlign:'left', verticalAlign:'center', margin:'20px'}}>\r\n            <tbody>\r\n                <tr>\r\n                    <td><h4>Education</h4></td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>Massachusetts Institute of Technology </b><br/>\r\n                        <div className=\"subtitle\">\r\n                            Relevant Coursework: \r\n                            &nbsp;\r\n                            <a href=\"https://mc.ai/6-141-robotics-science-systems-a-review/\" target=\"_blank\">\r\n                                Robotics: Science and Systems, \r\n                            </a>\r\n                            &nbsp;\r\n                            \r\n                            <a href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/\" target=\"_blank\">\r\n                                Performance Engineering of Software Systems*, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"http://web.mit.edu/6.033/www/general.shtml\" target=\"_blank\">\r\n                            Computer Systems Engineering*, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-005-software-construction-spring-2016/\" target=\"_blank\">\r\n                            Elements of Software Construction*, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"http://courses.csail.mit.edu/6.036/\" target=\"_blank\">\r\n                            Machine Learning, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://py.mit.edu/fall20\" target=\"_blank\">\r\n                            Fundamentals of Programming,\r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/\" target=\"_blank\">\r\n                            Introduction to Algorithms, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/\" target=\"_blank\">\r\n                            Introduction to Data Science, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://courses.csail.mit.edu/6.042/spring18/classinfo.shtml\" target=\"_blank\">\r\n                            Mathematics for Computer Science, \r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://www.eecs.mit.edu/academics-admissions/academic-information/subject-updates-spring-2020/6884\" target=\"_blank\">\r\n                            Computational Sensorimotor Learning*\r\n                            </a>\r\n                            &nbsp;\r\n                            <a href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2017/\" target=\"_blank\">\r\n                            Computation Structures*, \r\n                            </a>\r\n                            <br/>\r\n                            * - project based class\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>Cambridge, MA <br/>\r\n                        2015-2020</td>\r\n                </tr>\r\n                <tr>\r\n                    <td style={{paddingTop:'20px'}}><h4>Experience</h4></td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            <a href=\"http://darbelofflab.mit.edu/\" target=\"_blank\">\r\n                                MIT D’Arbeloff Laboratory\r\n                            </a>\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                        Undergraduate Research Assistant – UROP in robotics\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>Cambridge, MA <br/>\r\n                        Feb.‘19 – May ‘19</td>\r\n                </tr>\r\n                <tr>\r\n                    <ul>\r\n                        <li> Programmed a UR robotic arm as part of &nbsp;\r\n                            <a href=\"http://www.mit.edu/~nselby/teachbot.html\" target=\"_blank\">\r\n                                TeachBot - an interactive apprenticeship program.\r\n                            </a>\r\n                            </li>\r\n                        <li> Worked with ROS, MoveIt, ReactJs, ExpressJs.</li>\r\n                    </ul>\r\n                </tr>\r\n\r\n\r\n\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            Amazon Web Services\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Software Engineering Intern\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Berlin, Germany\r\n                        <br/>\r\n                        Jun.-Aug. 2017\r\n                        </td>\r\n                </tr>\r\n                <tr>\r\n                    <ul>\r\n                        <li> Improved the refund approval workflow by building a microservice that helps tech assistants review purchase history.</li>\r\n                        <li> Technologies used: AngularJS, ExpressJS, Java, Other internal AWS tools</li>\r\n                    </ul>\r\n                </tr>\r\n\r\n\r\n\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            <a href=\"http://acl.mit.edu/\" target=\"_blank\">\r\n                                MIT Aerospace Controls Lab\r\n                            </a>\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Undergraduate Research Assistant - UROP in Machine Learning/Robotics\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Cambridge, MA \r\n                        <br/>\r\n                        Feb. – May. ‘17\r\n                        </td>\r\n                </tr>\r\n                <tr>\r\n                    <ul>\r\n                        <li> Created a pipeline for automated acquisition, and processing of image datasets. \r\n</li>\r\n                        <li> Designed and tested algorithms for \r\n                            &nbsp;\r\n                            <a href=\"http://acl.mit.edu/projects/socially-acceptable-navigation\" target=\"_blank\">\r\n                                navigating through crowds while avoiding collisions.\r\n                            </a>\r\n                        </li>\r\n                    </ul>\r\n                </tr>\r\n\r\n\r\n\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            MIT\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Teacher Assistant for  &nbsp;\r\n                            <a href=\"https://py.mit.edu/fall20\" target=\"_blank\">\r\n                                6.009 (fundamentals of programming using Python)\r\n                            </a>\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Cambridge, MA \r\n                        <br/>\r\n                        Sep. ‘17 – May ‘18\r\n                        </td>\r\n                </tr>\r\n                <tr>\r\n                    <ul>\r\n                        <li>Helped teachers with assignments and student assistance.\r\n</li>\r\n                        <li>Explained concepts such as iteration, recursion, data structures in python, debugging techniques, writing efficient code, “the pythonic way” etc.\r\n</li>\r\n                    </ul>\r\n                </tr>\r\n\r\n\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            <a href=\"https://oneshore.com/\" target=\"_blank\">\r\n                                OneShore Energy GmbH\r\n                            </a>\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Software Engineering Intern\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Berlin, Germany\r\n                        <br/>\r\n                        Summer 2016\r\n                        </td>\r\n                </tr>\r\n                <tr>\r\n                    <ul>\r\n                        <li>Improved the data visualization process by developing a dashboard using Django on the backend and HTML/CSS/JS (jQuery, highcharts) on the frontend. \r\n</li>\r\n                        <li>Debugged and improved the efficiency of a simulation tool that predicts the performance of the electrical system (replaced and improved some algorithms and data structures used in the simulation and replaced some Python code with C++).\r\n</li>\r\n                    </ul>\r\n                </tr>\r\n\r\n\r\n                <tr>\r\n                    <td><h4>Leadership</h4></td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            <a href=\"https://gelp.mit.edu/\" target=\"_blank\">\r\n                                Gordon Engineering Leadership Program\r\n                            </a>\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Trainee\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>Cambridge, MA <br/>\r\n                        2017</td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            <a href=\"https://waveweekmoldova.wordpress.com/\" target=\"_blank\">\r\n                                Wave Week Moldova – a leadership program for students\r\n                            </a>\r\n\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Staff member and coach\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Chisinau, Moldova\r\n                        <br/>\r\n                        2012-2015\r\n                        </td>\r\n                </tr>\r\n\r\n\r\n                <tr>\r\n                    <td width=\"80%\">\r\n                        <b>\r\n                            English Debate Club at the \r\n                            &nbsp;\r\n                            <a href=\"https://americahouse.md/\" target=\"_blank\">\r\n                                American Resource Center\r\n                            </a>\r\n                        </b>\r\n\r\n                        <div className=\"subtitle\">\r\n                            Moderator and Coach\r\n                        </div>\r\n                        \r\n                        </td>\r\n                    <td width=\"20%\" style={{textAlign:\"center\"}}>\r\n                        Chisinau, Moldova\r\n                        <br/>\r\n                        2014-2015\r\n                        </td>\r\n                </tr>\r\n\r\n\r\n\r\n            </tbody>\r\n            </table>\r\n\r\n          <table id=\"t\" width=\"100%\" style={{textAlign:'left', verticalAlign:'center', margin:'20px'}}>\r\n                <tr>\r\n                    <td><h4>Skills</h4></td>\r\n                </tr>\r\n\r\n                <tr>\r\n                    <td width=\"20%\"> <b> Robotics: </b> </td>\r\n                    <td width=\"80%\" > \r\n                    ROS, Gazebo, Unity ML-Agents, MuJoCo, Pybullet, Fusion 360, Solidworks, Blender\r\n                    </td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"20%\"> <b> AI/ML: </b> </td>\r\n                    <td width=\"80%\" > \r\n                    Pytorch, reverb, CNNs, Autoencoders, Object Instance/Semantic segmentation\r\n                    </td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"20%\"> <b> WebDev: </b> </td>\r\n                    <td width=\"80%\" >\r\n                        Django, JQuery, ReactJS, ExpressJS, HTML/CSS/JS\r\n                    </td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"20%\"> <b> Other: </b> </td>\r\n                    <td width=\"80%\" >\r\n                        Linux, Docker, Arduino, Adobe Photoshop, git, vim, web scraping, violin, piano, drums\r\n                    </td>\r\n                </tr>\r\n                <tr>\r\n                    <td width=\"20%\"> <b> Languages: </b> </td>\r\n                    <td width=\"80%\" >\r\n                        Romanian (native), Russian (fluent), English (fluent), German (beginner)\r\n                    </td>\r\n                </tr>\r\n\r\n          </table>\r\n      </div>\r\n    )\r\n  }\r\n}\r\n\r\n","import React, { Component } from 'react'\r\nimport * as ReactBootstrap from 'react-bootstrap';\r\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\r\n\r\nexport default class Greeting extends Component {\r\n  render() {\r\n    return (\r\n        <div style={\r\n            {\r\n                background: \"url('/background.png')\",\r\n                backgroundSize: 'contain',\r\n                backgroundRepeat: 'no-repeat',\r\n                width:'100%',\r\n                height:'1000px',\r\n                color: 'white'\r\n            }\r\n        }>\r\n\r\n\r\n            <div className=\"introTextBox\">\r\n                <p>\r\n                    Hi, my name is Vlad and I'm a recent MIT graduate specializing in robotics and artificial intelligence. My main interest \r\n                    is in programming autonomous robots that can do useful physical tasks, such as picking irregular objects, harvesting and assembly. To that end, I study Control Theory, Reinforcement Learning, \r\n                    Immitation Learning, Computer Vision, Planning, and Simulation.\r\n                    <br/>\r\n                    If you are working on similar things, feel free to contact me: seremetv AT mit.edu\r\n                </p>\r\n            </div>\r\n        </div>\r\n    )\r\n  }\r\n}\r\n\r\n","import React from 'react';\nimport './App.css';\n// import Navbar from './components/navbar';\nimport Projects from './components/projects';\nimport Resume from './components/resume';\nimport Greeting from './components/greeting';\nimport {BrowserRouter as Router, Link, Route, NavLink} from 'react-router-dom';\nimport 'bootstrap/dist/css/bootstrap.min.css';\nimport * as ReactBootstrap from 'react-bootstrap';\nfunction App() {\n  var activelink=0;\n  return (\n    <Router basename={process.env.PUBLIC_URL}>\n      <div className=\"App main-container\">\n        <div className=\"navbar\">\n        <div className=\"left\">\n          <NavLink to=\"/\" exact style={\n            {\n              color: 'black',\n              textDecoration: 'none'\n            }\n          }>\n            <h2>Vlad Seremet</h2>\n          </NavLink>\n        </div>\n        <div className=\"right\" style={{fontSize:'20'}}>\n              <NavLink to=\"/resume\" className=\"navlink\" activeStyle={{borderBottom: '1px solid black'}} exact> \n                Resume \n              </NavLink>\n              <NavLink to=\"/projects\" className=\"navlink\" activeStyle={{borderBottom: 'solid 1px black'}}> \n              Projects\n              </NavLink>\n              <NavLink to=\"/blog\" className=\"navlink\" activeStyle={{borderBottom: 'solid 1px black'}}> \n                Blog\n              </NavLink>\n\n        </div>\n\n        </div>\n          <Route path='/' component={Greeting} exact/>\n          <Route path='/resume'   component={Resume} exact/>\n          <Route path='/projects' component={Projects}/>\n      </div>\n    </Router>\n  );\n}\n\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}